{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "591027f2",
   "metadata": {},
   "source": [
    "# Generate training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e0951c",
   "metadata": {},
   "source": [
    "### CISCO and Spaninsh Ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d8a9d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Caricamento file...\n",
      "   - Cisco loaded: (713, 32) righe\n",
      "   - Spanish ENS loaded: (213, 8) righe\n",
      "\n",
      "2. Indicizzazione e Aggregazione ENS (English)...\n",
      "   - Indicizzati 169 codici ENS univoci con testo in Inglese.\n",
      "\n",
      "3. Generazione coppie di training (Cisco -> ENS)...\n",
      "------------------------------\n",
      "RISULTATO FINALE\n",
      "------------------------------\n",
      "Totale coppie generate: 318\n",
      "Codici mancanti (nel file ENS): 28\n",
      "Esempio mancanti: ['op.cont.3', 'mp.per.9', 'mp.info.9', 'op.ext.9', 'op.exp.11']\n",
      "\n",
      "File salvato come: TrainAndTestData/trainingData/training_data_step1_cisco_ens.csv\n",
      "\n",
      "Anteprima dati:\n",
      "                                         anchor_text  \\\n",
      "0  Control Self-Assessments: Independent Control ...   \n",
      "1  Control Self-Assessments: Independent Control ...   \n",
      "\n",
      "                                         target_text  \n",
      "0  Metrics system. Taking into account the securi...  \n",
      "1  Certified components. Is the Catalog of Inform...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURAZIONE\n",
    "# ==============================================================================\n",
    "CISCO_PATH = \"Schemes/Cisco.csv\"         # Assicurati che il percorso sia corretto\n",
    "ENS_PATH = \"Schemes/SpanishENS.csv\"      # Assicurati che il percorso sia corretto\n",
    "OUTPUT_FILE = \"TrainAndTestData/trainingData/training_data_step1_cisco_ens.csv\"\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CARICAMENTO DATI\n",
    "# ==============================================================================\n",
    "print(\"1. Caricamento file...\")\n",
    "\n",
    "# Caricamento Cisco\n",
    "df_cisco = pd.read_csv(CISCO_PATH)\n",
    "print(f\"   - Cisco loaded: {df_cisco.shape} righe\")\n",
    "\n",
    "# Caricamento Spanish ENS (con separatore punto e virgola)\n",
    "# Usiamo dtype=str per evitare problemi con codici interpretati come numeri\n",
    "df_ens = pd.read_csv(ENS_PATH, sep=';', dtype=str)\n",
    "print(f\"   - Spanish ENS loaded: {df_ens.shape} righe\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. CREAZIONE LOOKUP TABLE (ENS INGLESE AGGREGATO)\n",
    "# ==============================================================================\n",
    "print(\"\\n2. Indicizzazione e Aggregazione ENS (English)...\")\n",
    "\n",
    "ens_lookup = {}\n",
    "\n",
    "# Raggruppiamo per codice perché nel CSV lo stesso codice può apparire più volte\n",
    "# (es. una riga per il titolo, una riga per le domande)\n",
    "grouped_ens = df_ens.groupby('Codice')\n",
    "\n",
    "for code, group in grouped_ens:\n",
    "    # Normalizza la chiave (codice)\n",
    "    if pd.isna(code): continue\n",
    "    clean_code = str(code).strip().lower()\n",
    "    \n",
    "    # Raccogli tutti i testi INGLESI disponibili per questo codice\n",
    "    texts = []\n",
    "    \n",
    "    for _, row in group.iterrows():\n",
    "        # Titolo Inglese\n",
    "        if 'Titolo_EN' in row and not pd.isna(row['Titolo_EN']):\n",
    "            t = str(row['Titolo_EN']).strip()\n",
    "            if t: texts.append(t)\n",
    "            \n",
    "        # Domande Inglese\n",
    "        if 'Domande_EN' in row and not pd.isna(row['Domande_EN']):\n",
    "            q = str(row['Domande_EN']).strip()\n",
    "            # Rimuovi i pipe '|' che separano le domande e sostituisci con spazio\n",
    "            q = q.replace('|', ' ')\n",
    "            if q: texts.append(q)\n",
    "            \n",
    "    # Unisci tutto il testo raccolto per questo controllo\n",
    "    full_text_en = \". \".join(texts)\n",
    "    \n",
    "    # Pulizia finale (spazi doppi, ecc)\n",
    "    full_text_en = re.sub(r'\\s+', ' ', full_text_en).strip()\n",
    "    \n",
    "    if full_text_en:\n",
    "        ens_lookup[clean_code] = full_text_en\n",
    "\n",
    "print(f\"   - Indicizzati {len(ens_lookup)} codici ENS univoci con testo in Inglese.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. MAPPING E GENERAZIONE DATASET\n",
    "# ==============================================================================\n",
    "print(\"\\n3. Generazione coppie di training (Cisco -> ENS)...\")\n",
    "\n",
    "training_data = []\n",
    "stats = {\n",
    "    \"matches\": 0,\n",
    "    \"missing\": set()\n",
    "}\n",
    "\n",
    "# Colonne Cisco dove cercare i riferimenti ENS\n",
    "ens_columns = ['Spanish ENS BASIC Control', 'Spanish ENS Medium Control', 'Spanish ENS High Control']\n",
    "\n",
    "for idx, row in df_cisco.iterrows():\n",
    "    # Anchor (Cisco)\n",
    "    # Combiniamo Titolo e Wording per la massima ricchezza semantica\n",
    "    if pd.isna(row['Control Reference']): continue\n",
    "    \n",
    "    anchor_id = str(row['Control Reference'])\n",
    "    title = str(row['Control Title']) if not pd.isna(row['Control Title']) else \"\"\n",
    "    wording = str(row['Control Wording']) if not pd.isna(row['Control Wording']) else \"\"\n",
    "    \n",
    "    anchor_text = f\"{title}: {wording}\".strip()\n",
    "    \n",
    "    # Target (ENS)\n",
    "    # Cerchiamo i codici nelle 3 colonne\n",
    "    target_codes = []\n",
    "    for col in ens_columns:\n",
    "        if col in row and not pd.isna(row[col]):\n",
    "            # Split per virgola in caso ci siano più controlli in una cella\n",
    "            codes_in_cell = str(row[col]).split(',')\n",
    "            target_codes.extend([c.strip().lower() for c in codes_in_cell])\n",
    "    \n",
    "    # Rimuovi duplicati e itera\n",
    "    unique_targets = set(target_codes)\n",
    "    \n",
    "    for t_code in unique_targets:\n",
    "        if not t_code: continue\n",
    "        \n",
    "        if t_code in ens_lookup:\n",
    "            # MATCH TROVATO\n",
    "            positive_text = ens_lookup[t_code]\n",
    "            \n",
    "            training_data.append({\n",
    "                'anchor_id': anchor_id,\n",
    "                'anchor_text': anchor_text,\n",
    "                'target_id': t_code,\n",
    "                'target_text': positive_text,\n",
    "                'source': 'Cisco',\n",
    "                'target': 'Spanish ENS (EN)'\n",
    "            })\n",
    "            stats[\"matches\"] += 1\n",
    "        else:\n",
    "            stats[\"missing\"].add(t_code)\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. SALVATAGGIO\n",
    "# ==============================================================================\n",
    "df_train = pd.DataFrame(training_data)\n",
    "df_train.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(\"RISULTATO FINALE\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Totale coppie generate: {len(df_train)}\")\n",
    "print(f\"Codici mancanti (nel file ENS): {len(stats['missing'])}\")\n",
    "if stats['missing']:\n",
    "    print(f\"Esempio mancanti: {list(stats['missing'])[:5]}\")\n",
    "print(f\"\\nFile salvato come: {OUTPUT_FILE}\")\n",
    "\n",
    "# Anteprima\n",
    "print(\"\\nAnteprima dati:\")\n",
    "print(df_train[['anchor_text', 'target_text']].head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cce0c1",
   "metadata": {},
   "source": [
    "### CISCO and Spaninsh Ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e03cbb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Caricamento e indicizzazione SecNumCloud...\n",
      "   - SecNumCloud caricato: 287 righe.\n",
      "   - Indicizzati 261 controlli SecNumCloud univoci (EN).\n",
      "\n",
      "2. Generazione coppie di training (Cisco -> SecNumCloud)...\n",
      "   - Cisco caricato: 713 righe.\n",
      "------------------------------\n",
      "RISULTATO STEP 2\n",
      "------------------------------\n",
      "Nuove coppie generate: 567\n",
      "Codici SecNumCloud non trovati: 33\n",
      "Esempio mancanti: ['12.7.d', '8.5.b', '5.3.e']...\n",
      "\n",
      "File salvato: TrainAndTestData/trainingData/training_data_step2_cisco_secNumCloud.csv\n",
      "\n",
      "Anteprima dati:\n",
      "  anchor_id target_id                                        target_text\n",
      "0     CCF 1  18.2.1.a  The service provider must document and impleme...\n",
      "1     CCF 1    18.4.a  The service provider must document and impleme...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURAZIONE PERCORSI\n",
    "# ==============================================================================\n",
    "# Percorsi basati sulla tua struttura di cartelle\n",
    "PATH_CISCO = \"Schemes/Cisco.csv\"\n",
    "PATH_SECNUM = \"Schemes/Secnumcloud.csv\"\n",
    "OUTPUT_FILE = \"TrainAndTestData/trainingData/training_data_step2_cisco_secNumCloud.csv\"\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CARICAMENTO E INDICIZZAZIONE SECNUMCLOUD\n",
    "# ==============================================================================\n",
    "print(\"1. Caricamento e indicizzazione SecNumCloud...\")\n",
    "\n",
    "try:\n",
    "    # Tentativo di lettura standard (virgola)\n",
    "    df_sec = pd.read_csv(PATH_SECNUM)\n",
    "    \n",
    "    # Verifica colonne critiche\n",
    "    if 'ID' not in df_sec.columns or 'Description_EN' not in df_sec.columns:\n",
    "        # Fallback: prova con punto e virgola se la prima lettura non ha parsato le colonne\n",
    "        df_sec = pd.read_csv(PATH_SECNUM, sep=';')\n",
    "\n",
    "    print(f\"   - SecNumCloud caricato: {len(df_sec)} righe.\")\n",
    "except Exception as e:\n",
    "    print(f\"   - ERRORE critico nel caricamento di SecNumCloud: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Creazione dizionario di lookup: ID (lowercase) -> Descrizione Inglese\n",
    "secnum_lookup = {}\n",
    "for _, row in df_sec.iterrows():\n",
    "    if pd.isna(row['ID']): continue\n",
    "    \n",
    "    # Normalizzazione ID (es: \"5.1.A \" -> \"5.1.a\")\n",
    "    clean_id = str(row['ID']).strip().lower()\n",
    "    \n",
    "    # Estrazione testo inglese\n",
    "    if 'Description_EN' in row and not pd.isna(row['Description_EN']):\n",
    "        desc = str(row['Description_EN']).strip()\n",
    "        secnum_lookup[clean_id] = desc\n",
    "\n",
    "print(f\"   - Indicizzati {len(secnum_lookup)} controlli SecNumCloud univoci (EN).\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. MAPPING CISCO -> SECNUMCLOUD\n",
    "# ==============================================================================\n",
    "print(\"\\n2. Generazione coppie di training (Cisco -> SecNumCloud)...\")\n",
    "\n",
    "df_cisco = pd.read_csv(PATH_CISCO)\n",
    "print(f\"   - Cisco caricato: {len(df_cisco)} righe.\")\n",
    "\n",
    "training_data = []\n",
    "stats = {\n",
    "    \"matches\": 0,\n",
    "    \"missing\": set()\n",
    "}\n",
    "\n",
    "# La colonna Cisco che contiene i riferimenti\n",
    "target_col = 'SecNumCloud Control'\n",
    "\n",
    "for idx, row in df_cisco.iterrows():\n",
    "    # Verifica se c'è un mapping\n",
    "    if target_col not in row or pd.isna(row[target_col]):\n",
    "        continue\n",
    "    \n",
    "    # 1. Preparazione Anchor (Cisco)\n",
    "    anchor_id = str(row['Control Reference'])\n",
    "    title = str(row['Control Title']) if not pd.isna(row['Control Title']) else \"\"\n",
    "    wording = str(row['Control Wording']) if not pd.isna(row['Control Wording']) else \"\"\n",
    "    \n",
    "    # Uniamo titolo e wording per dare contesto completo al modello\n",
    "    anchor_text = f\"{title}: {wording}\".strip()\n",
    "    \n",
    "    # 2. Preparazione Target (SecNumCloud IDs)\n",
    "    # Cisco può contenere liste tipo \"5.1.a, 5.2.b\" o usare newline\n",
    "    raw_refs = str(row[target_col]).replace('\\n', ',')\n",
    "    target_ids_list = [t.strip().lower() for t in raw_refs.split(',')]\n",
    "    \n",
    "    # 3. Matching\n",
    "    for t_id in set(target_ids_list): # set() per evitare duplicati sulla stessa riga\n",
    "        if not t_id: continue\n",
    "        \n",
    "        if t_id in secnum_lookup:\n",
    "            # MATCH TROVATO\n",
    "            positive_text = secnum_lookup[t_id]\n",
    "            \n",
    "            training_data.append({\n",
    "                'anchor_id': anchor_id,\n",
    "                'anchor_text': anchor_text,\n",
    "                'target_id': t_id,      # ID normalizzato\n",
    "                'target_text': positive_text,\n",
    "                'source': 'Cisco',\n",
    "                'target': 'SecNumCloud (EN)'\n",
    "            })\n",
    "            stats[\"matches\"] += 1\n",
    "        else:\n",
    "            # MATCH NON TROVATO (ID presente in Cisco ma non nel file SecNumCloud)\n",
    "            stats[\"missing\"].add(t_id)\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. SALVATAGGIO FILE STEP 2\n",
    "# ==============================================================================\n",
    "df_result = pd.DataFrame(training_data)\n",
    "df_result.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(\"RISULTATO STEP 2\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Nuove coppie generate: {len(df_result)}\")\n",
    "print(f\"Codici SecNumCloud non trovati: {len(stats['missing'])}\")\n",
    "if stats['missing']:\n",
    "    print(f\"Esempio mancanti: {list(stats['missing'])[:3]}...\")\n",
    "print(f\"\\nFile salvato: {OUTPUT_FILE}\")\n",
    "\n",
    "# Anteprima\n",
    "print(\"\\nAnteprima dati:\")\n",
    "print(df_result[['anchor_id', 'target_id', 'target_text']].head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39a25b2",
   "metadata": {},
   "source": [
    "### Cisco and BSIC5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c24fe34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Caricamento e indicizzazione BSI C5...\n",
      "   - BSI C5 JSON caricato. Totale elementi: 294\n",
      "   - Indicizzati 223 controlli BSI con testo.\n",
      "\n",
      "2. Generazione coppie di training (Cisco -> BSI C5)...\n",
      "   - Cisco CSV caricato: 713 righe.\n",
      "------------------------------\n",
      "RISULTATO STEP 3\n",
      "------------------------------\n",
      "Nuove coppie generate: 555\n",
      "Codici BSI citati in Cisco ma non trovati nel JSON: 0\n",
      "\n",
      "File salvato in: TrainAndTestData/trainingData/training_data_step3_cisco_bsi.csv\n",
      "\n",
      "Anteprima dati:\n",
      "  anchor_id target_id                                        target_text\n",
      "0     CCF 1     sp-01  Documentation, communication and provision of ...\n",
      "1     CCF 1     sp-02  Review and Approval of Policies and Instructio...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURAZIONE PERCORSI\n",
    "# ==============================================================================\n",
    "PATH_CISCO = \"Schemes/Cisco.csv\"\n",
    "PATH_BSI = \"Schemes/BSI-C5.json\"\n",
    "OUTPUT_FILE = \"TrainAndTestData/trainingData/training_data_step3_cisco_bsi.csv\"\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CARICAMENTO E INDICIZZAZIONE BSI C5 (JSON)\n",
    "# ==============================================================================\n",
    "print(\"1. Caricamento e indicizzazione BSI C5...\")\n",
    "\n",
    "try:\n",
    "    with open(PATH_BSI, 'r', encoding='utf-8') as f:\n",
    "        bsi_data = json.load(f)\n",
    "    print(f\"   - BSI C5 JSON caricato. Totale elementi: {len(bsi_data)}\")\n",
    "except Exception as e:\n",
    "    print(f\"   - ERRORE CRITICO nel caricamento del JSON BSI: {e}\")\n",
    "    # In un notebook, fermiamo l'esecuzione qui se il file non c'è\n",
    "    raise e\n",
    "\n",
    "bsi_lookup = {}\n",
    "\n",
    "for item in bsi_data:\n",
    "    # Verifica che il codice esista\n",
    "    if 'code' not in item or not item['code']:\n",
    "        continue\n",
    "        \n",
    "    # Normalizzazione ID (es. \"IAM-02\" -> \"iam-02\")\n",
    "    code = str(item['code']).strip().lower()\n",
    "    \n",
    "    # Costruzione del Testo Ricco\n",
    "    # Combiniamo Nome, Titolo Descrizione e Descrizione Estesa\n",
    "    name = item.get('name', '')\n",
    "    desc_title = item.get('descriptionTitle', '')\n",
    "    desc_ext = item.get('descriptionExtended', '')\n",
    "    \n",
    "    # Pulizia valori None/Null (che nel JSON appaiono come null)\n",
    "    if name is None: name = \"\"\n",
    "    if desc_title is None: desc_title = \"\"\n",
    "    if desc_ext is None: desc_ext = \"\"\n",
    "    \n",
    "    # Unione stringhe\n",
    "    full_text = f\"{name}: {desc_title} {desc_ext}\".strip()\n",
    "    \n",
    "    # Pulizia spazi extra e newline\n",
    "    full_text = re.sub(r'\\s+', ' ', full_text).strip()\n",
    "    \n",
    "    if full_text:\n",
    "        bsi_lookup[code] = full_text\n",
    "\n",
    "print(f\"   - Indicizzati {len(bsi_lookup)} controlli BSI con testo.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. MAPPING CISCO -> BSI C5\n",
    "# ==============================================================================\n",
    "print(\"\\n2. Generazione coppie di training (Cisco -> BSI C5)...\")\n",
    "\n",
    "try:\n",
    "    df_cisco = pd.read_csv(PATH_CISCO)\n",
    "    print(f\"   - Cisco CSV caricato: {len(df_cisco)} righe.\")\n",
    "except Exception as e:\n",
    "    print(f\"   - ERRORE CRITICO nel caricamento Cisco: {e}\")\n",
    "    raise e\n",
    "\n",
    "training_data = []\n",
    "stats = {\n",
    "    \"matches\": 0,\n",
    "    \"missing\": set()\n",
    "}\n",
    "\n",
    "# La colonna in Cisco si chiama esattamente \"BSI C5\"\n",
    "target_col = 'BSI C5'\n",
    "\n",
    "for idx, row in df_cisco.iterrows():\n",
    "    # Verifica se c'è un mapping in questa riga\n",
    "    if target_col not in row or pd.isna(row[target_col]):\n",
    "        continue\n",
    "        \n",
    "    # 1. Preparazione Anchor (Cisco)\n",
    "    anchor_id = str(row['Control Reference'])\n",
    "    title = str(row['Control Title']) if not pd.isna(row['Control Title']) else \"\"\n",
    "    wording = str(row['Control Wording']) if not pd.isna(row['Control Wording']) else \"\"\n",
    "    \n",
    "    anchor_text = f\"{title}: {wording}\".strip()\n",
    "    anchor_text = re.sub(r'\\s+', ' ', anchor_text) # pulizia veloce spazi\n",
    "    \n",
    "    # 2. Preparazione Target (BSI IDs)\n",
    "    # I codici in Cisco sono separati da virgola (es. \"OIS-01, SP-01\")\n",
    "    raw_refs = str(row[target_col]).replace('\\n', ',')\n",
    "    target_ids_list = [t.strip().lower() for t in raw_refs.split(',')]\n",
    "    \n",
    "    # 3. Matching\n",
    "    for t_id in set(target_ids_list): # set() per rimuovere duplicati\n",
    "        if not t_id: continue\n",
    "        \n",
    "        if t_id in bsi_lookup:\n",
    "            # MATCH TROVATO\n",
    "            positive_text = bsi_lookup[t_id]\n",
    "            \n",
    "            training_data.append({\n",
    "                'anchor_id': anchor_id,\n",
    "                'anchor_text': anchor_text,\n",
    "                'target_id': t_id,      # ID normalizzato\n",
    "                'target_text': positive_text,\n",
    "                'source': 'Cisco',\n",
    "                'target': 'BSI C5'\n",
    "            })\n",
    "            stats[\"matches\"] += 1\n",
    "        else:\n",
    "            # MATCH NON TROVATO\n",
    "            stats[\"missing\"].add(t_id)\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. SALVATAGGIO FILE STEP 3\n",
    "# ==============================================================================\n",
    "df_result = pd.DataFrame(training_data)\n",
    "df_result.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(\"RISULTATO STEP 3\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Nuove coppie generate: {len(df_result)}\")\n",
    "print(f\"Codici BSI citati in Cisco ma non trovati nel JSON: {len(stats['missing'])}\")\n",
    "if stats['missing']:\n",
    "    print(f\"Esempi codici mancanti: {list(stats['missing'])[:5]}...\")\n",
    "print(f\"\\nFile salvato in: {OUTPUT_FILE}\")\n",
    "\n",
    "# Anteprima\n",
    "print(\"\\nAnteprima dati:\")\n",
    "print(df_result[['anchor_id', 'target_id', 'target_text']].head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e376f4d",
   "metadata": {},
   "source": [
    "### Cisco and NewEUCS Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dfd76e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Caricamento e indicizzazione EUCS...\n",
      "   - EUCS caricato: 529 righe.\n",
      "   - Indicizzati 522 requisiti EUCS con testo.\n",
      "\n",
      "2. Generazione coppie di training (Cisco -> EUCS)...\n",
      "------------------------------\n",
      "RISULTATO STEP 4\n",
      "------------------------------\n",
      "Nuove coppie generate: 1272\n",
      "Codici EUCS non trovati: 14\n",
      "Esempio mancanti: ['cs-09.2', 'ps-04.4a', 'cs-03.2a', 'doc-06.3', 'ccm-02.5']...\n",
      "\n",
      "File salvato in: TrainAndTestData/trainingData/training_data_step4_cisco_eucs.csv\n",
      "\n",
      "Anteprima dati:\n",
      "  anchor_id target_id                                        target_text\n",
      "0     CCF 1  isp-02.5  In case of a delegation, the authorized bodies...\n",
      "1     CCF 1  ois-02.3  The CSP shall implement the mitigating measure...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURAZIONE PERCORSI\n",
    "# ==============================================================================\n",
    "PATH_CISCO = \"Schemes/Cisco.csv\"\n",
    "PATH_EUCS = \"Schemes/NewEucsRequirements_with_texts.csv\"\n",
    "OUTPUT_FILE = \"TrainAndTestData/trainingData/training_data_step4_cisco_eucs.csv\"\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CARICAMENTO E INDICIZZAZIONE EUCS\n",
    "# ==============================================================================\n",
    "print(\"1. Caricamento e indicizzazione EUCS...\")\n",
    "\n",
    "try:\n",
    "    # Tentativo primario con virgola (basato sul tuo 'head')\n",
    "    df_eucs = pd.read_csv(PATH_EUCS, sep=',', encoding='utf-8')\n",
    "    \n",
    "    # Verifica se le colonne chiave esistono, altrimenti riprova con ;\n",
    "    if 'EUCS Ref (Detailed)' not in df_eucs.columns:\n",
    "        print(\"   - Separatore virgola fallito, provo punto e virgola...\")\n",
    "        df_eucs = pd.read_csv(PATH_EUCS, sep=';', encoding='utf-8')\n",
    "        \n",
    "    print(f\"   - EUCS caricato: {len(df_eucs)} righe.\")\n",
    "except Exception as e:\n",
    "    print(f\"   - ERRORE CRITICO caricamento EUCS: {e}\")\n",
    "    # Fallback encoding latin-1 se utf-8 fallisce\n",
    "    try:\n",
    "        df_eucs = pd.read_csv(PATH_EUCS, sep=',', encoding='latin-1')\n",
    "        print(\"   - EUCS caricato con encoding latin-1.\")\n",
    "    except:\n",
    "        raise e\n",
    "\n",
    "eucs_lookup = {}\n",
    "\n",
    "for _, row in df_eucs.iterrows():\n",
    "    # La chiave è il riferimento dettagliato (es. OIS-01.1)\n",
    "    if 'EUCS Ref (Detailed)' not in row or pd.isna(row['EUCS Ref (Detailed)']):\n",
    "        continue\n",
    "        \n",
    "    # Normalizzazione ID\n",
    "    code = str(row['EUCS Ref (Detailed)']).strip().lower()\n",
    "    \n",
    "    # Estrazione testo\n",
    "    text = \"\"\n",
    "    if 'EUCS Text' in row and not pd.isna(row['EUCS Text']):\n",
    "        text = str(row['EUCS Text']).strip()\n",
    "        # Pulizia caratteri strani (es. newline o pipe)\n",
    "        text = text.replace('\\n', ' ').replace('|', ' ')\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "    if code and text:\n",
    "        eucs_lookup[code] = text\n",
    "\n",
    "print(f\"   - Indicizzati {len(eucs_lookup)} requisiti EUCS con testo.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. MAPPING CISCO -> EUCS\n",
    "# ==============================================================================\n",
    "print(\"\\n2. Generazione coppie di training (Cisco -> EUCS)...\")\n",
    "\n",
    "df_cisco = pd.read_csv(PATH_CISCO)\n",
    "training_data = []\n",
    "stats = {\n",
    "    \"matches\": 0,\n",
    "    \"missing\": set()\n",
    "}\n",
    "\n",
    "# Colonne target in Cisco (ne abbiamo 3 per l'EUCS)\n",
    "target_columns = [\n",
    "    'EUCS Basic Control', \n",
    "    'EUCS Substantial Control', \n",
    "    'EUCS High Control'\n",
    "]\n",
    "\n",
    "for idx, row in df_cisco.iterrows():\n",
    "    # 1. Anchor (Cisco)\n",
    "    anchor_id = str(row['Control Reference'])\n",
    "    title = str(row['Control Title']) if not pd.isna(row['Control Title']) else \"\"\n",
    "    wording = str(row['Control Wording']) if not pd.isna(row['Control Wording']) else \"\"\n",
    "    anchor_text = f\"{title}: {wording}\".strip()\n",
    "    anchor_text = re.sub(r'\\s+', ' ', anchor_text)\n",
    "    \n",
    "    # 2. Raccolta Target IDs da tutte e 3 le colonne\n",
    "    target_ids_list = []\n",
    "    for col in target_columns:\n",
    "        if col in row and not pd.isna(row[col]):\n",
    "            # Split per virgola o newline\n",
    "            raw_val = str(row[col]).replace('\\n', ',')\n",
    "            codes = [c.strip().lower() for c in raw_val.split(',')]\n",
    "            target_ids_list.extend(codes)\n",
    "            \n",
    "    # 3. Matching\n",
    "    for t_id in set(target_ids_list): # set() rimuove duplicati tra le colonne\n",
    "        if not t_id: continue\n",
    "        \n",
    "        # Correzione typo comune (visto nel file Cisco: OSI -> OIS)\n",
    "        if t_id.startswith('osi-'):\n",
    "            t_id = t_id.replace('osi-', 'ois-')\n",
    "\n",
    "        if t_id in eucs_lookup:\n",
    "            # MATCH TROVATO\n",
    "            positive_text = eucs_lookup[t_id]\n",
    "            \n",
    "            training_data.append({\n",
    "                'anchor_id': anchor_id,\n",
    "                'anchor_text': anchor_text,\n",
    "                'target_id': t_id,\n",
    "                'target_text': positive_text,\n",
    "                'source': 'Cisco',\n",
    "                'target': 'EUCS'\n",
    "            })\n",
    "            stats[\"matches\"] += 1\n",
    "        else:\n",
    "            stats[\"missing\"].add(t_id)\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. SALVATAGGIO FILE STEP 4\n",
    "# ==============================================================================\n",
    "df_result = pd.DataFrame(training_data)\n",
    "df_result.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(\"RISULTATO STEP 4\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Nuove coppie generate: {len(df_result)}\")\n",
    "print(f\"Codici EUCS non trovati: {len(stats['missing'])}\")\n",
    "if stats['missing']:\n",
    "    print(f\"Esempio mancanti: {list(stats['missing'])[:5]}...\")\n",
    "print(f\"\\nFile salvato in: {OUTPUT_FILE}\")\n",
    "\n",
    "# Anteprima\n",
    "print(\"\\nAnteprima dati:\")\n",
    "print(df_result[['anchor_id', 'target_id', 'target_text']].head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9b4604",
   "metadata": {},
   "source": [
    "### NewEucs and Fabasoft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a15ffae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Caricamento EUCS e indicizzazione via BSI C5...\n",
      "   - EUCS caricato: 529 righe.\n",
      "   - Colonne trovate: ['EUCS Category', 'EUCS Control (2022)', 'Control_Code', 'EUCS Ref (Detailed)', 'EUCS Text', 'EUCS Ass. Level', 'Code', 'C5.2020 GERMANY', 'SecNumCloud FRANCE', 'ISO 27002', 'ISO 27017']\n",
      "   - Indicizzati 110 codici BSI che puntano a 576 requisiti EUCS.\n",
      "\n",
      "2. Elaborazione Fabasoft Metrics -> EUCS (via BSI)...\n",
      "   - Fabasoft caricato: 57 righe.\n",
      "   - Colonna Riferimenti Fabasoft: 'Possible Control ID\n",
      "Scheme'\n",
      "------------------------------\n",
      "RISULTATO STEP 5\n",
      "------------------------------\n",
      "Nuove coppie generate: 206\n",
      "Codici BSI in Fabasoft non mappati in EUCS: 1\n",
      "\n",
      "File salvato in: TrainAndTestData/trainingData/training_data_step5_fabasoft_eucs.csv\n",
      "                         anchor_id target_id\n",
      "0  NumberOfKnownLowVulnerabilities  OPS-17.1\n",
      "1  NumberOfKnownLowVulnerabilities  OPS-17.2\n",
      "2  NumberOfKnownLowVulnerabilities  OPS-17.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURAZIONE PERCORSI\n",
    "# ==============================================================================\n",
    "PATH_EUCS = \"Schemes/NewEucsRequirements_with_texts.csv\"\n",
    "PATH_FABASOFT = \"Schemes/fabasoftMetrics.csv\"\n",
    "OUTPUT_FILE = \"TrainAndTestData/trainingData/training_data_step5_fabasoft_eucs.csv\"\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CARICAMENTO E INDICIZZAZIONE EUCS (VIA BSI C5)\n",
    "# ==============================================================================\n",
    "print(\"1. Caricamento EUCS e indicizzazione via BSI C5...\")\n",
    "\n",
    "df_eucs = pd.DataFrame()\n",
    "\n",
    "# Lettura file con pulizia colonne\n",
    "try:\n",
    "    # Usiamo engine python per gestire CSV complessi\n",
    "    df_eucs = pd.read_csv(PATH_EUCS, sep=',', encoding='utf-8', engine='python', on_bad_lines='skip')\n",
    "    \n",
    "    # --- FIX CRUCIALE: RIMUOVIAMO SPAZI DAI NOMI DELLE COLONNE ---\n",
    "    df_eucs.columns = df_eucs.columns.str.strip()\n",
    "    \n",
    "    print(f\"   - EUCS caricato: {len(df_eucs)} righe.\")\n",
    "    print(f\"   - Colonne trovate: {df_eucs.columns.tolist()}\") # Debug\n",
    "except Exception as e:\n",
    "    print(f\"   - Errore caricamento EUCS (utf-8): {e}\")\n",
    "    # Fallback latin-1 e separatore ;\n",
    "    try:\n",
    "        df_eucs = pd.read_csv(PATH_EUCS, sep=';', encoding='latin-1', engine='python', on_bad_lines='skip')\n",
    "        df_eucs.columns = df_eucs.columns.str.strip()\n",
    "        print(f\"   - EUCS caricato (latin-1, ;): {len(df_eucs)} righe.\")\n",
    "    except Exception as e2:\n",
    "        print(f\"   - ERRORE CRITICO FILE EUCS: {e2}\")\n",
    "\n",
    "# Individuazione colonne target dopo lo strip\n",
    "col_bsi = 'C5.2020 GERMANY'\n",
    "col_eucs_id = 'EUCS Ref (Detailed)'\n",
    "col_eucs_text = 'EUCS Text'\n",
    "\n",
    "# Verifica esistenza colonna BSI\n",
    "if col_bsi not in df_eucs.columns:\n",
    "    print(f\"   - ATTENZIONE: Colonna '{col_bsi}' non trovata anche dopo la pulizia!\")\n",
    "    # Tentativo di trovarla parzialmente\n",
    "    candidates = [c for c in df_eucs.columns if \"C5\" in c and \"GERMANY\" in c]\n",
    "    if candidates:\n",
    "        col_bsi = candidates[0]\n",
    "        print(f\"   - Usando colonna alternativa: '{col_bsi}'\")\n",
    "\n",
    "bsi_to_eucs = {}\n",
    "count_mapped = 0\n",
    "\n",
    "if not df_eucs.empty and col_bsi in df_eucs.columns:\n",
    "    for _, row in df_eucs.iterrows():\n",
    "        # Estrazione dati EUCS\n",
    "        e_id = str(row[col_eucs_id]).strip() if col_eucs_id in row and not pd.isna(row[col_eucs_id]) else \"\"\n",
    "        e_text = str(row[col_eucs_text]).strip() if col_eucs_text in row and not pd.isna(row[col_eucs_text]) else \"\"\n",
    "        \n",
    "        if not e_id or not e_text: \n",
    "            continue\n",
    "            \n",
    "        # Estrazione dati BSI (Link)\n",
    "        bsi_refs = str(row[col_bsi]) if not pd.isna(row[col_bsi]) else \"\"\n",
    "        if not bsi_refs: \n",
    "            continue\n",
    "            \n",
    "        # Split per newline o virgola (es. \"OIS-01\\nOIS-03\")\n",
    "        refs = re.split(r'[\\n,]', bsi_refs)\n",
    "        \n",
    "        for r in refs:\n",
    "            clean_bsi = r.strip().lower()\n",
    "            if clean_bsi:\n",
    "                if clean_bsi not in bsi_to_eucs:\n",
    "                    bsi_to_eucs[clean_bsi] = []\n",
    "                bsi_to_eucs[clean_bsi].append((e_id, e_text))\n",
    "                count_mapped += 1\n",
    "\n",
    "print(f\"   - Indicizzati {len(bsi_to_eucs)} codici BSI che puntano a {count_mapped} requisiti EUCS.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. ELABORAZIONE FABASOFT METRICS\n",
    "# ==============================================================================\n",
    "print(\"\\n2. Elaborazione Fabasoft Metrics -> EUCS (via BSI)...\")\n",
    "\n",
    "try:\n",
    "    df_fab = pd.read_csv(PATH_FABASOFT, engine='python', on_bad_lines='skip')\n",
    "    print(f\"   - Fabasoft caricato: {len(df_fab)} righe.\")\n",
    "except Exception as e:\n",
    "    print(f\"   - ERRORE Fabasoft: {e}\")\n",
    "    df_fab = pd.DataFrame()\n",
    "\n",
    "# Trova colonna riferimenti (contiene \"Possible Control ID\")\n",
    "ref_col = None\n",
    "for col in df_fab.columns:\n",
    "    if \"Possible Control ID\" in col:\n",
    "        ref_col = col\n",
    "        break\n",
    "if not ref_col and not df_fab.empty:\n",
    "    ref_col = df_fab.columns[-1] # Fallback\n",
    "\n",
    "print(f\"   - Colonna Riferimenti Fabasoft: '{ref_col}'\")\n",
    "\n",
    "training_data = []\n",
    "stats = {\"matches\": 0, \"missing_bsi_link\": set()}\n",
    "\n",
    "if not df_fab.empty and ref_col and bsi_to_eucs:\n",
    "    for idx, row in df_fab.iterrows():\n",
    "        # Anchor (Metric)\n",
    "        m_id = str(row.get('ID', ''))\n",
    "        m_name = str(row.get('Name', ''))\n",
    "        m_desc = str(row.get('Description', ''))\n",
    "        \n",
    "        # Pulizia nan\n",
    "        if m_id.lower() == 'nan': m_id = \"\"\n",
    "        if m_name.lower() == 'nan': m_name = \"\"\n",
    "        if m_desc.lower() == 'nan': m_desc = \"\"\n",
    "        \n",
    "        anchor_parts = [p for p in [m_id, m_name, m_desc] if p]\n",
    "        anchor_text = \": \".join(anchor_parts)\n",
    "        anchor_text = re.sub(r'\\s+', ' ', anchor_text).strip()\n",
    "        \n",
    "        if not anchor_text: continue\n",
    "        \n",
    "        # Estrazione Referenze BSI da Fabasoft\n",
    "        raw_refs = str(row[ref_col]) if not pd.isna(row[ref_col]) else \"\"\n",
    "        lines = raw_refs.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            target_bsi = None\n",
    "            \n",
    "            # Cerca \"BSI C5 OPS-22\"\n",
    "            if \"BSI C5\" in line:\n",
    "                target_bsi = re.sub(r'BSI\\s+C5\\s+', '', line, flags=re.IGNORECASE).strip()\n",
    "            \n",
    "            if target_bsi:\n",
    "                clean_bsi = target_bsi.lower()\n",
    "                \n",
    "                # BRIDGE: Fabasoft(BSI) -> EUCS(BSI)\n",
    "                if clean_bsi in bsi_to_eucs:\n",
    "                    eucs_targets = bsi_to_eucs[clean_bsi]\n",
    "                    for (e_id, e_text) in eucs_targets:\n",
    "                        training_data.append({\n",
    "                            'anchor_id': m_id,\n",
    "                            'anchor_text': anchor_text,\n",
    "                            'target_id': e_id,\n",
    "                            'target_text': e_text,\n",
    "                            'source': 'Fabasoft Metrics',\n",
    "                            'target': 'EUCS (via BSI C5)'\n",
    "                        })\n",
    "                        stats[\"matches\"] += 1\n",
    "                else:\n",
    "                    stats[\"missing_bsi_link\"].add(clean_bsi)\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. SALVATAGGIO\n",
    "# ==============================================================================\n",
    "df_result = pd.DataFrame(training_data, columns=['anchor_id', 'anchor_text', 'target_id', 'target_text', 'source', 'target'])\n",
    "df_result.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(\"RISULTATO STEP 5\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Nuove coppie generate: {len(df_result)}\")\n",
    "if stats['missing_bsi_link']:\n",
    "    print(f\"Codici BSI in Fabasoft non mappati in EUCS: {len(stats['missing_bsi_link'])}\")\n",
    "print(f\"\\nFile salvato in: {OUTPUT_FILE}\")\n",
    "\n",
    "if not df_result.empty:\n",
    "    print(df_result[['anchor_id', 'target_id']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ffead3",
   "metadata": {},
   "source": [
    "### BSIC5 and Fabasoft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e09564b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Caricamento e indicizzazione BSI C5...\n",
      "   - BSI C5 JSON caricato. Totale elementi: 294\n",
      "   - Indicizzati 223 controlli BSI con testo.\n",
      "\n",
      "2. Elaborazione Fabasoft Metrics -> BSI C5 (Direct)...\n",
      "   - Fabasoft caricato: 57 righe.\n",
      "   - Colonna Riferimenti Fabasoft: 'Possible Control ID\n",
      "Scheme'\n",
      "------------------------------\n",
      "RISULTATO STEP 6\n",
      "------------------------------\n",
      "Nuove coppie generate: 27\n",
      "Codici BSI mancanti: 1 (es: ['idm-11'])\n",
      "\n",
      "File salvato in: TrainAndTestData/trainingData/training_data_step6_fabasoft_bsi.csv\n",
      "                            anchor_id target_id\n",
      "0     NumberOfKnownLowVulnerabilities    ops-22\n",
      "1     NumberOfKnownLowVulnerabilities    pss-02\n",
      "2  NumberOfKnownMediumVulnerabilities    ops-22\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURAZIONE PERCORSI\n",
    "# ==============================================================================\n",
    "PATH_FABASOFT = \"Schemes/fabasoftMetrics.csv\"\n",
    "PATH_BSI = \"Schemes/BSI-C5.json\"\n",
    "OUTPUT_FILE = \"TrainAndTestData/trainingData/training_data_step6_fabasoft_bsi.csv\"\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CARICAMENTO E INDICIZZAZIONE BSI C5 (JSON)\n",
    "# ==============================================================================\n",
    "print(\"1. Caricamento e indicizzazione BSI C5...\")\n",
    "\n",
    "try:\n",
    "    with open(PATH_BSI, 'r', encoding='utf-8') as f:\n",
    "        bsi_data = json.load(f)\n",
    "    print(f\"   - BSI C5 JSON caricato. Totale elementi: {len(bsi_data)}\")\n",
    "except Exception as e:\n",
    "    print(f\"   - ERRORE CRITICO BSI: {e}\")\n",
    "    exit()\n",
    "\n",
    "bsi_lookup = {}\n",
    "\n",
    "for item in bsi_data:\n",
    "    if 'code' not in item or not item['code']:\n",
    "        continue\n",
    "        \n",
    "    # Normalizzazione ID (es. \"OPS-22\" -> \"ops-22\")\n",
    "    code = str(item['code']).strip().lower()\n",
    "    \n",
    "    # Costruzione Testo Ricco\n",
    "    name = item.get('name', '') or \"\"\n",
    "    desc_title = item.get('descriptionTitle', '') or \"\"\n",
    "    desc_ext = item.get('descriptionExtended', '') or \"\"\n",
    "    \n",
    "    full_text = f\"{name}: {desc_title} {desc_ext}\".strip()\n",
    "    full_text = re.sub(r'\\s+', ' ', full_text).strip()\n",
    "    \n",
    "    if full_text:\n",
    "        bsi_lookup[code] = full_text\n",
    "\n",
    "print(f\"   - Indicizzati {len(bsi_lookup)} controlli BSI con testo.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. ELABORAZIONE FABASOFT METRICS -> BSI DIRECT\n",
    "# ==============================================================================\n",
    "print(\"\\n2. Elaborazione Fabasoft Metrics -> BSI C5 (Direct)...\")\n",
    "\n",
    "try:\n",
    "    # Engine python per gestire header multi-riga e quote\n",
    "    df_fab = pd.read_csv(PATH_FABASOFT, engine='python', on_bad_lines='skip')\n",
    "    print(f\"   - Fabasoft caricato: {len(df_fab)} righe.\")\n",
    "except Exception as e:\n",
    "    print(f\"   - ERRORE Fabasoft: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Trova la colonna giusta (quella che contiene \"Possible Control ID\")\n",
    "ref_col = None\n",
    "for col in df_fab.columns:\n",
    "    if \"Possible Control ID\" in col:\n",
    "        ref_col = col\n",
    "        break\n",
    "# Fallback\n",
    "if not ref_col and not df_fab.empty:\n",
    "    ref_col = df_fab.columns[-1]\n",
    "\n",
    "print(f\"   - Colonna Riferimenti Fabasoft: '{ref_col}'\")\n",
    "\n",
    "training_data = []\n",
    "stats = {\"matches\": 0, \"missing\": set()}\n",
    "\n",
    "if not df_fab.empty and ref_col:\n",
    "    for idx, row in df_fab.iterrows():\n",
    "        # 1. Anchor (Metrica)\n",
    "        m_id = str(row.get('ID', ''))\n",
    "        m_name = str(row.get('Name', ''))\n",
    "        m_desc = str(row.get('Description', ''))\n",
    "        \n",
    "        # Pulizia stringhe \"nan\"\n",
    "        if m_id.lower() == 'nan': m_id = \"\"\n",
    "        if m_name.lower() == 'nan': m_name = \"\"\n",
    "        if m_desc.lower() == 'nan': m_desc = \"\"\n",
    "        \n",
    "        anchor_parts = [p for p in [m_id, m_name, m_desc] if p]\n",
    "        anchor_text = \": \".join(anchor_parts)\n",
    "        anchor_text = re.sub(r'\\s+', ' ', anchor_text).strip()\n",
    "        \n",
    "        if not anchor_text: continue\n",
    "        \n",
    "        # 2. Estrazione Riferimenti BSI\n",
    "        raw_refs = str(row[ref_col]) if not pd.isna(row[ref_col]) else \"\"\n",
    "        \n",
    "        # Il contenuto è tipo \"BSI C5 OPS-22\\nBSI C5 PSS-02\"\n",
    "        lines = raw_refs.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            \n",
    "            # Regex per estrarre il codice dopo \"BSI C5\"\n",
    "            # Cerca \"BSI C5\" (case insensitive) seguito da spazi opzionali e poi il codice\n",
    "            match = re.search(r'BSI\\s*C5\\s*([\\w-]+)', line, re.IGNORECASE)\n",
    "            \n",
    "            if match:\n",
    "                target_code = match.group(1).lower() # es. \"ops-22\"\n",
    "                \n",
    "                # 3. Matching\n",
    "                if target_code in bsi_lookup:\n",
    "                    positive_text = bsi_lookup[target_code]\n",
    "                    \n",
    "                    training_data.append({\n",
    "                        'anchor_id': m_id,\n",
    "                        'anchor_text': anchor_text,\n",
    "                        'target_id': target_code,\n",
    "                        'target_text': positive_text,\n",
    "                        'source': 'Fabasoft Metrics',\n",
    "                        'target': 'BSI C5'\n",
    "                    })\n",
    "                    stats[\"matches\"] += 1\n",
    "                else:\n",
    "                    stats[\"missing\"].add(target_code)\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. SALVATAGGIO\n",
    "# ==============================================================================\n",
    "df_result = pd.DataFrame(training_data)\n",
    "df_result.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(\"RISULTATO STEP 6\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Nuove coppie generate: {len(df_result)}\")\n",
    "if stats['missing']:\n",
    "    print(f\"Codici BSI mancanti: {len(stats['missing'])} (es: {list(stats['missing'])[:3]})\")\n",
    "print(f\"\\nFile salvato in: {OUTPUT_FILE}\")\n",
    "\n",
    "if not df_result.empty:\n",
    "    print(df_result[['anchor_id', 'target_id']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83633965",
   "metadata": {},
   "source": [
    "### newEUCS and BSIC5 no old data overlap from oldEUCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21be404f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Caricamento Old EUCS (Blacklist)...\n",
      "   - Trovati 70 codici 'Old EUCS' da escludere dal training.\n",
      "\n",
      "2. Indicizzazione BSI C5...\n",
      "   - Indicizzati 223 controlli BSI.\n",
      "\n",
      "3. Generazione coppie NewEUCS -> BSI (Filtrate)...\n",
      "------------------------------\n",
      "RISULTATO STEP 7 (FILTRATO)\n",
      "------------------------------\n",
      "Controlli Old EUCS trovati (Blacklist): 70\n",
      "Coppie potenziali scartate perché nel Test Set: 0\n",
      "Nuove coppie valide generate: 569\n",
      "File salvato in: TrainAndTestData/trainingData/training_data_step7_neweucs_bsi_filtered.csv\n",
      "\n",
      "Anteprima:\n",
      "  anchor_id target_id\n",
      "0  OIS-01.1    ois-01\n",
      "1  OIS-01.2    ois-01\n",
      "2  OIS-01.3    ois-01\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURAZIONE PERCORSI\n",
    "# ==============================================================================\n",
    "PATH_NEW_EUCS = \"Schemes/NewEucsRequirements_with_texts.csv\"\n",
    "PATH_OLD_EUCS = \"Schemes/OldEucsRequirements.csv\"\n",
    "PATH_BSI = \"Schemes/BSI-C5.json\"\n",
    "OUTPUT_FILE = \"TrainAndTestData/trainingData/training_data_step7_neweucs_bsi_filtered.csv\"\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CARICAMENTO \"OLD EUCS\" (DA ESCLUDERE)\n",
    "# ==============================================================================\n",
    "print(\"1. Caricamento Old EUCS (Blacklist)...\")\n",
    "old_eucs_codes = set()\n",
    "\n",
    "try:\n",
    "    # Provo a leggere il vecchio file\n",
    "    df_old = pd.read_csv(PATH_OLD_EUCS)\n",
    "    \n",
    "    # Identifico la colonna ID (dal tuo head sembra 'controlId')\n",
    "    col_id_old = None\n",
    "    for c in df_old.columns:\n",
    "        if 'controlId' in c or 'ID' in c:\n",
    "            col_id_old = c\n",
    "            break\n",
    "            \n",
    "    if col_id_old:\n",
    "        # Normalizzo e salvo nel set\n",
    "        old_eucs_codes = set(df_old[col_id_old].dropna().astype(str).str.strip().str.lower())\n",
    "        print(f\"   - Trovati {len(old_eucs_codes)} codici 'Old EUCS' da escludere dal training.\")\n",
    "    else:\n",
    "        print(\"   - ATTENZIONE: Colonna ID non trovata in Old EUCS. Nessun filtro applicato!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   - Errore caricamento Old EUCS: {e}\")\n",
    "    print(\"   - Procedo senza filtrare (rischio data leakage se questo file serviva per il test).\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. INDICIZZAZIONE BSI C5 (TARGET)\n",
    "# ==============================================================================\n",
    "print(\"\\n2. Indicizzazione BSI C5...\")\n",
    "bsi_lookup = {}\n",
    "\n",
    "try:\n",
    "    with open(PATH_BSI, 'r', encoding='utf-8') as f:\n",
    "        bsi_data = json.load(f)\n",
    "        \n",
    "    for item in bsi_data:\n",
    "        if 'code' not in item: continue\n",
    "        \n",
    "        # ID BSI normalizzato\n",
    "        code = str(item['code']).strip().lower()\n",
    "        \n",
    "        # Testo Ricco\n",
    "        name = item.get('name', '') or \"\"\n",
    "        desc_t = item.get('descriptionTitle', '') or \"\"\n",
    "        desc_e = item.get('descriptionExtended', '') or \"\"\n",
    "        \n",
    "        full_text = f\"{name}: {desc_t} {desc_e}\".strip()\n",
    "        full_text = re.sub(r'\\s+', ' ', full_text).strip()\n",
    "        \n",
    "        if full_text:\n",
    "            bsi_lookup[code] = full_text\n",
    "\n",
    "    print(f\"   - Indicizzati {len(bsi_lookup)} controlli BSI.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   - ERRORE CRITICO BSI: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. MAPPING NEW_EUCS -> BSI (CON FILTRO)\n",
    "# ==============================================================================\n",
    "print(\"\\n3. Generazione coppie NewEUCS -> BSI (Filtrate)...\")\n",
    "\n",
    "training_data = []\n",
    "stats = {\n",
    "    \"total_candidates\": 0,\n",
    "    \"excluded_by_filter\": 0,\n",
    "    \"matches\": 0,\n",
    "    \"missing_bsi\": 0\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Caricamento New EUCS con pulizia colonne\n",
    "    df_new = pd.read_csv(PATH_NEW_EUCS, sep=',', engine='python', on_bad_lines='skip')\n",
    "    # Se fallisce virgola, prova ;\n",
    "    if 'C5.2020 GERMANY' not in df_new.columns and len(df_new.columns) < 5:\n",
    "         df_new = pd.read_csv(PATH_NEW_EUCS, sep=';', engine='python', on_bad_lines='skip')\n",
    "         \n",
    "    df_new.columns = df_new.columns.str.strip() # Rimuove spazi dai nomi colonne\n",
    "\n",
    "    # Colonne chiave\n",
    "    col_new_id = 'EUCS Ref (Detailed)' # ID univoco del controllo EUCS\n",
    "    col_new_text = 'EUCS Text'\n",
    "    col_bsi_ref = 'C5.2020 GERMANY'\n",
    "\n",
    "    # Verifica colonne\n",
    "    if col_new_id not in df_new.columns or col_bsi_ref not in df_new.columns:\n",
    "        print(f\"   - Colonne mancanti in New EUCS. Trovate: {df_new.columns.tolist()}\")\n",
    "        exit()\n",
    "\n",
    "    for idx, row in df_new.iterrows():\n",
    "        # 1. Analisi Anchor (EUCS)\n",
    "        eucs_id_raw = str(row[col_new_id])\n",
    "        eucs_id_clean = eucs_id_raw.strip().lower()\n",
    "        \n",
    "        eucs_text = str(row[col_new_text]) if col_new_text in row else \"\"\n",
    "        eucs_text = re.sub(r'\\s+', ' ', eucs_text).strip()\n",
    "\n",
    "        if pd.isna(row[col_new_id]) or not eucs_text:\n",
    "            continue\n",
    "\n",
    "        # 2. FILTRO ANTI-LEAKAGE\n",
    "        # Se questo ID EUCS è presente nel vecchio set (OldEucs), SALTALO.\n",
    "        if eucs_id_clean in old_eucs_codes:\n",
    "            stats[\"excluded_by_filter\"] += 1\n",
    "            continue\n",
    "\n",
    "        # 3. Analisi Target (BSI)\n",
    "        bsi_refs_raw = str(row[col_bsi_ref])\n",
    "        if pd.isna(row[col_bsi_ref]) or not bsi_refs_raw.strip():\n",
    "            continue\n",
    "\n",
    "        # Split refs (newline o virgola)\n",
    "        refs = re.split(r'[\\n,]', bsi_refs_raw)\n",
    "        \n",
    "        for r in refs:\n",
    "            stats[\"total_candidates\"] += 1\n",
    "            target_bsi = r.strip().lower()\n",
    "            \n",
    "            if not target_bsi: continue\n",
    "\n",
    "            if target_bsi in bsi_lookup:\n",
    "                # MATCH!\n",
    "                target_text = bsi_lookup[target_bsi]\n",
    "                \n",
    "                training_data.append({\n",
    "                    'anchor_id': eucs_id_raw, # Mantengo ID originale per leggibilità\n",
    "                    'anchor_text': eucs_text,\n",
    "                    'target_id': target_bsi,\n",
    "                    'target_text': target_text,\n",
    "                    'source': 'NewEUCS (Filtered)',\n",
    "                    'target': 'BSI C5'\n",
    "                })\n",
    "                stats[\"matches\"] += 1\n",
    "            else:\n",
    "                stats[\"missing_bsi\"] += 1\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   - ERRORE durante il processing: {e}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. SALVATAGGIO\n",
    "# ==============================================================================\n",
    "df_result = pd.DataFrame(training_data)\n",
    "df_result.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(\"RISULTATO STEP 7 (FILTRATO)\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Controlli Old EUCS trovati (Blacklist): {len(old_eucs_codes)}\")\n",
    "print(f\"Coppie potenziali scartate perché nel Test Set: {stats['excluded_by_filter']}\")\n",
    "print(f\"Nuove coppie valide generate: {len(df_result)}\")\n",
    "print(f\"File salvato in: {OUTPUT_FILE}\")\n",
    "\n",
    "if not df_result.empty:\n",
    "    print(\"\\nAnteprima:\")\n",
    "    print(df_result[['anchor_id', 'target_id']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3e13f6",
   "metadata": {},
   "source": [
    "## Merge to creare train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3681397d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inizio creazione del MASTER DATASET COMPLETO...\n",
      "   - Lettura: training_data_step1_cisco_ens.csv ... OK (318 righe)\n",
      "   - Lettura: training_data_step2_cisco_secNumCloud.csv ... OK (567 righe)\n",
      "   - Lettura: training_data_step3_cisco_bsi.csv ... OK (555 righe)\n",
      "   - Lettura: training_data_step4_cisco_eucs.csv ... OK (1272 righe)\n",
      "   - Lettura: training_data_step5_fabasoft_eucs.csv ... OK (206 righe)\n",
      "   - Lettura: training_data_step6_fabasoft_bsi.csv ... OK (27 righe)\n",
      "   - Lettura: training_data_step7_neweucs_bsi_filtered.csv ... OK (569 righe)\n",
      "\n",
      "Mescolamento e Pulizia Finale...\n",
      "   - Rimossi 6 duplicati esatti.\n",
      "------------------------------\n",
      "RISULTATO FINALE (MASTER DATASET)\n",
      "------------------------------\n",
      "Totale righe nel dataset di training: 3508\n",
      "\n",
      "Distribuzione per Schema Target:\n",
      "target\n",
      "EUCS                 1270\n",
      "BSI C5               1151\n",
      "SecNumCloud (EN)      563\n",
      "Spanish ENS (EN)      318\n",
      "EUCS (via BSI C5)     206\n",
      "Name: count, dtype: int64\n",
      "\n",
      "File MASTER salvato in: TrainAndTestData/trainingData/MASTER_TRAINING_DATA.csv\n",
      "\n",
      "Anteprima finale:\n",
      "  source            target anchor_id target_id\n",
      "0  Cisco              EUCS   CCF 286   pm-02.3\n",
      "1  Cisco  Spanish ENS (EN)    CCF 59   mp.si.2\n",
      "2  Cisco  SecNumCloud (EN)    CCF 28     6.5.a\n",
      "3  Cisco  SecNumCloud (EN)     CCF 8    18.1.a\n",
      "4  Cisco              EUCS   CCF 175  iam-07.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURAZIONE\n",
    "# ==============================================================================\n",
    "DATA_DIR = \"TrainAndTestData/trainingData/\"\n",
    "OUTPUT_MASTER = \"TrainAndTestData/trainingData/MASTER_TRAINING_DATA.csv\"\n",
    "\n",
    "# Lista completa dei 7 file (Tutti gli step fatti finora)\n",
    "FILES_TO_MERGE = [\n",
    "    \"training_data_step1_cisco_ens.csv\",            # Cisco -> Spanish ENS\n",
    "    \"training_data_step2_cisco_secNumCloud.csv\",    # Cisco -> SecNumCloud\n",
    "    \"training_data_step3_cisco_bsi.csv\",            # Cisco -> BSI C5\n",
    "    \"training_data_step4_cisco_eucs.csv\",           # Cisco -> EUCS (Originale)\n",
    "    \"training_data_step5_fabasoft_eucs.csv\",        # Fabasoft -> EUCS\n",
    "    \"training_data_step6_fabasoft_bsi.csv\",         # Fabasoft -> BSI C5\n",
    "    \"training_data_step7_neweucs_bsi_filtered.csv\"  # NewEUCS -> BSI (Filtrato Old)\n",
    "]\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. UNIONE (MERGE)\n",
    "# ==============================================================================\n",
    "print(\"Inizio creazione del MASTER DATASET COMPLETO...\")\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for filename in FILES_TO_MERGE:\n",
    "    file_path = os.path.join(DATA_DIR, filename)\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"   - Lettura: {filename} ...\", end=\"\")\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            # Aggiungiamo colonna origine per tracciabilità\n",
    "            df['original_file'] = filename\n",
    "            df_list.append(df)\n",
    "            print(f\" OK ({len(df)} righe)\")\n",
    "        except Exception as e:\n",
    "            print(f\" ERRORE: {e}\")\n",
    "    else:\n",
    "        print(f\"   - ATTENZIONE: File mancante: {filename}\")\n",
    "\n",
    "if not df_list:\n",
    "    print(\"ERRORE: Nessun file trovato!\")\n",
    "    exit()\n",
    "\n",
    "# Concatenazione\n",
    "master_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. SHUFFLE E PULIZIA\n",
    "# ==============================================================================\n",
    "print(\"\\nMescolamento e Pulizia Finale...\")\n",
    "\n",
    "# 1. Shuffle (Mescolamento casuale)\n",
    "# shuffle al 100% per rompere l'ordine dei file\n",
    "master_df = master_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# 2. Rimozione duplicati esatti\n",
    "# Se una coppia è identica (stessa ancora, stesso target), la teniamo una volta sola\n",
    "initial_len = len(master_df)\n",
    "master_df.drop_duplicates(subset=['anchor_text', 'target_text'], inplace=True)\n",
    "dedup_len = len(master_df)\n",
    "\n",
    "if initial_len != dedup_len:\n",
    "    print(f\"   - Rimossi {initial_len - dedup_len} duplicati esatti.\")\n",
    "\n",
    "# 3. Pulizia NaN\n",
    "master_df.dropna(subset=['anchor_text', 'target_text'], inplace=True)\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. SALVATAGGIO\n",
    "# ==============================================================================\n",
    "master_df.to_csv(OUTPUT_MASTER, index=False, encoding='utf-8')\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(\"RISULTATO FINALE (MASTER DATASET)\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Totale righe nel dataset di training: {len(master_df)}\")\n",
    "print(\"\\nDistribuzione per Schema Target:\")\n",
    "# Mostra quante coppie abbiamo per ogni tipo di target\n",
    "print(master_df['target'].value_counts())\n",
    "\n",
    "print(f\"\\nFile MASTER salvato in: {OUTPUT_MASTER}\")\n",
    "print(\"\\nAnteprima finale:\")\n",
    "print(master_df[['source', 'target', 'anchor_id', 'target_id']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6619f5",
   "metadata": {},
   "source": [
    "## Generate the 2 test data\n",
    "\n",
    "one is the old test data from the old paper (oldEucs and medinaMetrics) and the other (the one that is made by old data but it was not use for testing) is oldEucs and bsic5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29d51364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Caricamento Schemi Base...\n",
      "   - Old EUCS caricato: 70 righe.\n",
      "   - BSI C5 caricato: 223 controlli.\n",
      "\n",
      "2. Generazione Test Set 1 (Medina -> Old EUCS)...\n",
      "   - Creato TrainAndTestData/testData/test_set_1_medina_oldeucs.csv con 179 coppie.\n",
      "\n",
      "3. Generazione Test Set 2 (Old EUCS -> BSI C5)...\n",
      "   - Creato TrainAndTestData/testData/test_set_2_oldeucs_bsi.csv con 140 coppie (Match euristico su ID).\n",
      "------------------------------\n",
      "TEST SET GENERATION COMPLETED\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURAZIONE PERCORSI\n",
    "# ==============================================================================\n",
    "PATH_OLD_EUCS = \"Schemes/OldEucsRequirements.csv\"\n",
    "PATH_MEDINA = \"Schemes/medinaMetrics.csv\"\n",
    "PATH_BSI = \"Schemes/BSI-C5.json\"\n",
    "\n",
    "# Output folder\n",
    "TEST_DIR = \"TrainAndTestData/testData/\"\n",
    "if not os.path.exists(TEST_DIR):\n",
    "    os.makedirs(TEST_DIR)\n",
    "\n",
    "OUT_TEST_1 = os.path.join(TEST_DIR, \"test_set_1_medina_oldeucs.csv\")\n",
    "OUT_TEST_2 = os.path.join(TEST_DIR, \"test_set_2_oldeucs_bsi.csv\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CARICAMENTO E INDICIZZAZIONE DATI BASE\n",
    "# ==============================================================================\n",
    "print(\"1. Caricamento Schemi Base...\")\n",
    "\n",
    "# A) Old EUCS (Target per Test 1, Anchor per Test 2)\n",
    "# -------------------------------------------------\n",
    "try:\n",
    "    df_old_eucs = pd.read_csv(PATH_OLD_EUCS)\n",
    "    # Cerco colonna ID e Descrizione\n",
    "    col_oe_id = next((c for c in df_old_eucs.columns if 'controlId' in c or 'ID' in c), None)\n",
    "    col_oe_desc = next((c for c in df_old_eucs.columns if 'description' in c.lower()), None)\n",
    "    \n",
    "    if col_oe_id and col_oe_desc:\n",
    "        print(f\"   - Old EUCS caricato: {len(df_old_eucs)} righe.\")\n",
    "    else:\n",
    "        print(\"   - ERRORE: Colonne ID/Description non trovate in Old EUCS.\")\n",
    "        exit()\n",
    "        \n",
    "    # Creo dizionario Old EUCS: ID -> Text\n",
    "    oldeucs_lookup = {}\n",
    "    for _, row in df_old_eucs.iterrows():\n",
    "        oid = str(row[col_oe_id]).strip()\n",
    "        otext = str(row[col_oe_desc]).strip()\n",
    "        oldeucs_lookup[oid] = otext\n",
    "        # Normalizzo ID per lookup (lowercase)\n",
    "        oldeucs_lookup[oid.lower()] = otext\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"   - ERRORE Old EUCS: {e}\")\n",
    "    exit()\n",
    "\n",
    "# B) BSI C5 (Target per Test 2)\n",
    "# -----------------------------\n",
    "bsi_lookup = {}\n",
    "try:\n",
    "    with open(PATH_BSI, 'r', encoding='utf-8') as f:\n",
    "        bsi_data = json.load(f)\n",
    "        \n",
    "    for item in bsi_data:\n",
    "        if 'code' not in item: continue\n",
    "        code = str(item['code']).strip().lower()\n",
    "        \n",
    "        name = item.get('name', '') or \"\"\n",
    "        desc = item.get('descriptionTitle', '') or \"\"\n",
    "        full_text = f\"{name}: {desc}\".strip()\n",
    "        \n",
    "        bsi_lookup[code] = full_text\n",
    "    print(f\"   - BSI C5 caricato: {len(bsi_lookup)} controlli.\")\n",
    "except Exception as e:\n",
    "    print(f\"   - ERRORE BSI: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. GENERAZIONE TEST SET 1: Medina Metrics -> Old EUCS\n",
    "# ==============================================================================\n",
    "print(\"\\n2. Generazione Test Set 1 (Medina -> Old EUCS)...\")\n",
    "# Logica: In medinaMetrics.csv c'è una colonna 'controlId' che punta a Old EUCS\n",
    "\n",
    "test1_data = []\n",
    "try:\n",
    "    df_medina = pd.read_csv(PATH_MEDINA)\n",
    "    \n",
    "    # Identifico colonne\n",
    "    col_med_id = 'ID'\n",
    "    col_med_desc = 'description'\n",
    "    col_med_ref = 'controlId' # Punta a Old EUCS\n",
    "    \n",
    "    matches_1 = 0\n",
    "    for idx, row in df_medina.iterrows():\n",
    "        # Anchor: Metrica\n",
    "        m_id = str(row[col_med_id])\n",
    "        m_desc = str(row[col_med_desc])\n",
    "        anchor_text = f\"{m_id}: {m_desc}\" # ID + Descrizione\n",
    "        \n",
    "        # Target: Old EUCS\n",
    "        ref_id = str(row[col_med_ref]).strip()\n",
    "        \n",
    "        # Cerco il testo del target nel dizionario Old EUCS\n",
    "        # Provo exact match o lowercase\n",
    "        target_text = oldeucs_lookup.get(ref_id) or oldeucs_lookup.get(ref_id.lower())\n",
    "        \n",
    "        if target_text:\n",
    "            test1_data.append({\n",
    "                'anchor_id': m_id,\n",
    "                'anchor_text': anchor_text,\n",
    "                'target_id': ref_id,\n",
    "                'target_text': target_text,\n",
    "                'source': 'Medina Metrics',\n",
    "                'target': 'Old EUCS'\n",
    "            })\n",
    "            matches_1 += 1\n",
    "            \n",
    "    # Salvataggio\n",
    "    pd.DataFrame(test1_data).to_csv(OUT_TEST_1, index=False)\n",
    "    print(f\"   - Creato {OUT_TEST_1} con {matches_1} coppie.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   - ERRORE Test Set 1: {e}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. GENERAZIONE TEST SET 2: Old EUCS -> BSI C5\n",
    "# ==============================================================================\n",
    "print(\"\\n3. Generazione Test Set 2 (Old EUCS -> BSI C5)...\")\n",
    "# Logica: Dobbiamo capire come Old EUCS mappa su BSI.\n",
    "# Se Old EUCS non ha una colonna esplicita \"BSI Mapping\", \n",
    "# dobbiamo inferirlo dal nome (es. OPS-05.3H -> OPS-05?) \n",
    "# O forse il mapping è implicito perché usano codici simili?\n",
    "#\n",
    "# ANALISI: In OldEucsRequirements.csv (dal tuo head) vedo solo ID e Description.\n",
    "# Non vedo una colonna di mapping verso BSI.\n",
    "# TUTTAVIA, l'ID stesso (es. \"OPS-05.3H\") sembra derivare dai domini BSI (OPS, IAM, etc).\n",
    "#\n",
    "# Se non hai un file di mapping esplicito \"OldEUCS -> BSI\", \n",
    "# l'unica strategia automatica è provare a matchare il prefisso del codice.\n",
    "# Es: \"OPS-05.3H\" -> cerco \"OPS-05\" in BSI?\n",
    "#\n",
    "# Oppure, se vuoi testare se il modello capisce la semantica, possiamo creare un file\n",
    "# dove il target è il controllo BSI che ha l'ID base corrispondente.\n",
    "\n",
    "test2_data = []\n",
    "matches_2 = 0\n",
    "\n",
    "for oe_id, oe_text in oldeucs_lookup.items():\n",
    "    # Provo a estrarre una radice BSI-compatibile dall'ID Old EUCS\n",
    "    # Es: \"OPS-05.3H\" -> \"ops-05\"\n",
    "    # Regex: Prendi lettere e numeri fino al secondo trattino o punto\n",
    "    \n",
    "    # Tentativo euristico: (CAT)-(NUM)\n",
    "    match = re.match(r'([A-Za-z]+-\\d+)', oe_id) # Cattura \"OPS-05\" da \"OPS-05.3H\"\n",
    "    \n",
    "    if match:\n",
    "        potential_bsi_code = match.group(1).lower() # \"ops-05\"\n",
    "        \n",
    "        if potential_bsi_code in bsi_lookup:\n",
    "            bsi_text = bsi_lookup[potential_bsi_code]\n",
    "            \n",
    "            test2_data.append({\n",
    "                'anchor_id': oe_id,      # Old EUCS ID\n",
    "                'anchor_text': oe_text,  # Old EUCS Text\n",
    "                'target_id': potential_bsi_code, # BSI ID\n",
    "                'target_text': bsi_text, # BSI Text\n",
    "                'source': 'Old EUCS',\n",
    "                'target': 'BSI C5 (Inferred)'\n",
    "            })\n",
    "            matches_2 += 1\n",
    "\n",
    "# Salvataggio\n",
    "pd.DataFrame(test2_data).to_csv(OUT_TEST_2, index=False)\n",
    "print(f\"   - Creato {OUT_TEST_2} con {matches_2} coppie (Match euristico su ID).\")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(\"TEST SET GENERATION COMPLETED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "251c31e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento dataset...\n",
      "   - Fabasoft: 37 descrizioni caricate.\n",
      "   - Medina: 183 descrizioni caricate.\n",
      "\n",
      "Inizio confronto (Similarity Check)...\n",
      "1. Exact Matches (stessa descrizione normalizzata): 0\n",
      "2. Fuzzy Matching in corso (potrebbe richiedere qualche secondo)...\n",
      "\n",
      "------------------------------------------------\n",
      "RISULTATO ANALISI\n",
      "------------------------------------------------\n",
      "✅ NESSUNA sovrapposizione rilevata.\n",
      "   Puoi usare tranquillamente Fabasoft nel Training e Medina nel Test.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURAZIONE\n",
    "# ==============================================================================\n",
    "PATH_FABASOFT = \"Schemes/fabasoftMetrics.csv\"\n",
    "PATH_MEDINA = \"Schemes/medinaMetrics.csv\"\n",
    "\n",
    "# Soglia di similarità (0.9 = 90% simile)\n",
    "SIMILARITY_THRESHOLD = 0.9 \n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CARICAMENTO E PULIZIA\n",
    "# ==============================================================================\n",
    "print(\"Caricamento dataset...\")\n",
    "\n",
    "# Fabasoft\n",
    "try:\n",
    "    df_fab = pd.read_csv(PATH_FABASOFT, engine='python', on_bad_lines='skip')\n",
    "    # Fabasoft ha la descrizione nella colonna 'Description'\n",
    "    fab_descs = df_fab['Description'].dropna().astype(str).tolist()\n",
    "    print(f\"   - Fabasoft: {len(fab_descs)} descrizioni caricate.\")\n",
    "except Exception as e:\n",
    "    print(f\"   - Errore Fabasoft: {e}\")\n",
    "    fab_descs = []\n",
    "\n",
    "# Medina\n",
    "try:\n",
    "    df_med = pd.read_csv(PATH_MEDINA)\n",
    "    # Medina ha la descrizione nella colonna 'description'\n",
    "    med_descs = df_med['description'].dropna().astype(str).tolist()\n",
    "    print(f\"   - Medina: {len(med_descs)} descrizioni caricate.\")\n",
    "except Exception as e:\n",
    "    print(f\"   - Errore Medina: {e}\")\n",
    "    med_descs = []\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. ANALISI SOVRAPPOSIZIONE\n",
    "# ==============================================================================\n",
    "print(\"\\nInizio confronto (Similarity Check)...\")\n",
    "\n",
    "def normalize(text):\n",
    "    # Rimuove caratteri non alfanumerici e mette in lowercase\n",
    "    return re.sub(r'[^a-z0-9]', '', text.lower())\n",
    "\n",
    "# Set per lookup veloce (Exact Match)\n",
    "fab_norm_set = set(normalize(d) for d in fab_descs)\n",
    "med_norm_set = set(normalize(d) for d in med_descs)\n",
    "\n",
    "# A. Exact Matches\n",
    "intersection = fab_norm_set.intersection(med_norm_set)\n",
    "print(f\"1. Exact Matches (stessa descrizione normalizzata): {len(intersection)}\")\n",
    "\n",
    "# B. Fuzzy Matches (Controllo più lento ma accurato)\n",
    "potential_leaks = []\n",
    "\n",
    "# Per non fare un prodotto cartesiano enorme, controlliamo solo se i set sono piccoli (<1000)\n",
    "if len(fab_descs) < 500 and len(med_descs) < 500:\n",
    "    print(\"2. Fuzzy Matching in corso (potrebbe richiedere qualche secondo)...\")\n",
    "    for f_desc in fab_descs:\n",
    "        for m_desc in med_descs:\n",
    "            # Calcola similarità\n",
    "            ratio = SequenceMatcher(None, f_desc, m_desc).ratio()\n",
    "            if ratio >= SIMILARITY_THRESHOLD:\n",
    "                potential_leaks.append((f_desc, m_desc, ratio))\n",
    "else:\n",
    "    print(\"2. Fuzzy Matching saltato (dataset troppo grandi per confronto rapido).\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. REPORT\n",
    "# ==============================================================================\n",
    "print(\"\\n------------------------------------------------\")\n",
    "print(\"RISULTATO ANALISI\")\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "if len(intersection) == 0 and len(potential_leaks) == 0:\n",
    "    print(\"✅ NESSUNA sovrapposizione rilevata.\")\n",
    "    print(\"   Puoi usare tranquillamente Fabasoft nel Training e Medina nel Test.\")\n",
    "else:\n",
    "    print(f\"⚠️ ATTENZIONE: Rilevata possibile sovrapposizione!\")\n",
    "    print(f\"   - Duplicati esatti: {len(intersection)}\")\n",
    "    print(f\"   - Duplicati simili (>90%): {len(potential_leaks)}\")\n",
    "    \n",
    "    if potential_leaks:\n",
    "        print(\"\\nEsempi di similarità trovata:\")\n",
    "        for f, m, r in potential_leaks[:5]:\n",
    "            print(f\"   Fabasoft: {f[:50]}...\")\n",
    "            print(f\"   Medina:   {m[:50]}...\")\n",
    "            print(f\"   Score:    {r:.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e5b459",
   "metadata": {},
   "source": [
    "# Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5648a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anchor_id</th>\n",
       "      <th>anchor_text</th>\n",
       "      <th>target_id</th>\n",
       "      <th>target_text</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>original_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCF 286</td>\n",
       "      <td>Supplier Management Program: A supplier manage...</td>\n",
       "      <td>pm-02.3</td>\n",
       "      <td>Following the risk assessment of a subservice ...</td>\n",
       "      <td>Cisco</td>\n",
       "      <td>EUCS</td>\n",
       "      <td>training_data_step4_cisco_eucs.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CCF 59</td>\n",
       "      <td>Device and Hardware Transfer: Devices and hard...</td>\n",
       "      <td>mp.si.2</td>\n",
       "      <td>Cryptography</td>\n",
       "      <td>Cisco</td>\n",
       "      <td>Spanish ENS (EN)</td>\n",
       "      <td>training_data_step1_cisco_ens.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCF 28</td>\n",
       "      <td>SDLC Methodology: A formal [The Organization] ...</td>\n",
       "      <td>6.5.a</td>\n",
       "      <td>The service provider must document an estimate...</td>\n",
       "      <td>Cisco</td>\n",
       "      <td>SecNumCloud (EN)</td>\n",
       "      <td>training_data_step2_cisco_secNumCloud.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CCF 8</td>\n",
       "      <td>Cybersecurity Legal and Regulatory Requirement...</td>\n",
       "      <td>18.1.a</td>\n",
       "      <td>The service provider must identify the legal, ...</td>\n",
       "      <td>Cisco</td>\n",
       "      <td>SecNumCloud (EN)</td>\n",
       "      <td>training_data_step2_cisco_secNumCloud.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CCF 175</td>\n",
       "      <td>Password Management &amp; Configuration: Passwords...</td>\n",
       "      <td>iam-07.2</td>\n",
       "      <td>The access to all environments of the CSP shal...</td>\n",
       "      <td>Cisco</td>\n",
       "      <td>EUCS</td>\n",
       "      <td>training_data_step4_cisco_eucs.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3503</th>\n",
       "      <td>CCF 133</td>\n",
       "      <td>Policies and Standards over Metadata: Policies...</td>\n",
       "      <td>ops-12</td>\n",
       "      <td>LOGGING AND MONITORING – IDENTIFICATION OF EVE...</td>\n",
       "      <td>Cisco</td>\n",
       "      <td>BSI C5</td>\n",
       "      <td>training_data_step3_cisco_bsi.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3504</th>\n",
       "      <td>CCF 246</td>\n",
       "      <td>Deletion of PII: [The Organization] either del...</td>\n",
       "      <td>pi-03</td>\n",
       "      <td>SECURE DELETION OF DATA: CSC data is securely ...</td>\n",
       "      <td>Cisco</td>\n",
       "      <td>BSI C5</td>\n",
       "      <td>training_data_step3_cisco_bsi.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3505</th>\n",
       "      <td>CCF 315</td>\n",
       "      <td>Infrastructure &amp; Application patching: Infrast...</td>\n",
       "      <td>12.11.a</td>\n",
       "      <td>The service provider must document and impleme...</td>\n",
       "      <td>Cisco</td>\n",
       "      <td>SecNumCloud (EN)</td>\n",
       "      <td>training_data_step2_cisco_secNumCloud.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3506</th>\n",
       "      <td>PSS-03.2</td>\n",
       "      <td>The CSP shall validate the functionality of th...</td>\n",
       "      <td>pss-10</td>\n",
       "      <td>Software Defined Networking: If the Cloud Serv...</td>\n",
       "      <td>NewEUCS (Filtered)</td>\n",
       "      <td>BSI C5</td>\n",
       "      <td>training_data_step7_neweucs_bsi_filtered.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3507</th>\n",
       "      <td>OPS-17.4</td>\n",
       "      <td>The CSP shall mandate in its policies and proc...</td>\n",
       "      <td>ops-18</td>\n",
       "      <td>MANAGING VULNERABILITIES, MALFUNCTIONS AND ERR...</td>\n",
       "      <td>NewEUCS (Filtered)</td>\n",
       "      <td>BSI C5</td>\n",
       "      <td>training_data_step7_neweucs_bsi_filtered.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3508 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     anchor_id                                        anchor_text target_id  \\\n",
       "0      CCF 286  Supplier Management Program: A supplier manage...   pm-02.3   \n",
       "1       CCF 59  Device and Hardware Transfer: Devices and hard...   mp.si.2   \n",
       "2       CCF 28  SDLC Methodology: A formal [The Organization] ...     6.5.a   \n",
       "3        CCF 8  Cybersecurity Legal and Regulatory Requirement...    18.1.a   \n",
       "4      CCF 175  Password Management & Configuration: Passwords...  iam-07.2   \n",
       "...        ...                                                ...       ...   \n",
       "3503   CCF 133  Policies and Standards over Metadata: Policies...    ops-12   \n",
       "3504   CCF 246  Deletion of PII: [The Organization] either del...     pi-03   \n",
       "3505   CCF 315  Infrastructure & Application patching: Infrast...   12.11.a   \n",
       "3506  PSS-03.2  The CSP shall validate the functionality of th...    pss-10   \n",
       "3507  OPS-17.4  The CSP shall mandate in its policies and proc...    ops-18   \n",
       "\n",
       "                                            target_text              source  \\\n",
       "0     Following the risk assessment of a subservice ...               Cisco   \n",
       "1                                          Cryptography               Cisco   \n",
       "2     The service provider must document an estimate...               Cisco   \n",
       "3     The service provider must identify the legal, ...               Cisco   \n",
       "4     The access to all environments of the CSP shal...               Cisco   \n",
       "...                                                 ...                 ...   \n",
       "3503  LOGGING AND MONITORING – IDENTIFICATION OF EVE...               Cisco   \n",
       "3504  SECURE DELETION OF DATA: CSC data is securely ...               Cisco   \n",
       "3505  The service provider must document and impleme...               Cisco   \n",
       "3506  Software Defined Networking: If the Cloud Serv...  NewEUCS (Filtered)   \n",
       "3507  MANAGING VULNERABILITIES, MALFUNCTIONS AND ERR...  NewEUCS (Filtered)   \n",
       "\n",
       "                target                                 original_file  \n",
       "0                 EUCS            training_data_step4_cisco_eucs.csv  \n",
       "1     Spanish ENS (EN)             training_data_step1_cisco_ens.csv  \n",
       "2     SecNumCloud (EN)     training_data_step2_cisco_secNumCloud.csv  \n",
       "3     SecNumCloud (EN)     training_data_step2_cisco_secNumCloud.csv  \n",
       "4                 EUCS            training_data_step4_cisco_eucs.csv  \n",
       "...                ...                                           ...  \n",
       "3503            BSI C5             training_data_step3_cisco_bsi.csv  \n",
       "3504            BSI C5             training_data_step3_cisco_bsi.csv  \n",
       "3505  SecNumCloud (EN)     training_data_step2_cisco_secNumCloud.csv  \n",
       "3506            BSI C5  training_data_step7_neweucs_bsi_filtered.csv  \n",
       "3507            BSI C5  training_data_step7_neweucs_bsi_filtered.csv  \n",
       "\n",
       "[3508 rows x 7 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\"TrainAndTestData/trainingData/MASTER_TRAINING_DATA.csv\")\n",
    "test1 = pd.read_csv(\"TrainAndTestData/testData/test_set_1_medina_oldeucs.csv\")\n",
    "test2 = pd.read_csv(\"TrainAndTestData/testData/test_set_2_oldeucs_bsi.csv\") \n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "098b1abe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anchor_id</th>\n",
       "      <th>anchor_text</th>\n",
       "      <th>target_id</th>\n",
       "      <th>target_text</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1: This metric is used to assess if the antima...</td>\n",
       "      <td>OPS-05.3H</td>\n",
       "      <td>The CSP shall automatically monitor the system...</td>\n",
       "      <td>Medina Metrics</td>\n",
       "      <td>Old EUCS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2: This metric is used to assess if the antima...</td>\n",
       "      <td>OPS-05.3H</td>\n",
       "      <td>The CSP shall automatically monitor the system...</td>\n",
       "      <td>Medina Metrics</td>\n",
       "      <td>Old EUCS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3: This metric is used to assess if backups ar...</td>\n",
       "      <td>OPS-07.2H</td>\n",
       "      <td>In order to check the proper application of th...</td>\n",
       "      <td>Medina Metrics</td>\n",
       "      <td>Old EUCS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3: This metric is used to assess if backups ar...</td>\n",
       "      <td>OPS-09.2H</td>\n",
       "      <td>When the backup data is transmitted to a remot...</td>\n",
       "      <td>Medina Metrics</td>\n",
       "      <td>Old EUCS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4: This metric is used to assess the configure...</td>\n",
       "      <td>OPS-07.2H</td>\n",
       "      <td>In order to check the proper application of th...</td>\n",
       "      <td>Medina Metrics</td>\n",
       "      <td>Old EUCS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>163</td>\n",
       "      <td>163: Where is access control monitored and reg...</td>\n",
       "      <td>PS-02.8H</td>\n",
       "      <td>The access control policy shall include loggin...</td>\n",
       "      <td>Medina Metrics</td>\n",
       "      <td>Old EUCS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>164</td>\n",
       "      <td>164: Which tool is used for monitoring the lis...</td>\n",
       "      <td>ISP-03.5H</td>\n",
       "      <td>The list of exceptions shall be automatically ...</td>\n",
       "      <td>Medina Metrics</td>\n",
       "      <td>Old EUCS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>165</td>\n",
       "      <td>165: Which processes are in place for event de...</td>\n",
       "      <td>OPS-12.2H</td>\n",
       "      <td>The CSP shall automatically monitor that event...</td>\n",
       "      <td>Medina Metrics</td>\n",
       "      <td>Old EUCS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>166</td>\n",
       "      <td>166: How are changes in role and right managem...</td>\n",
       "      <td>CCM-05.1H</td>\n",
       "      <td>The CSP shall define roles and rights accordin...</td>\n",
       "      <td>Medina Metrics</td>\n",
       "      <td>Old EUCS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>167</td>\n",
       "      <td>167: This metric checks whether the policy doc...</td>\n",
       "      <td>ISP-02.4H</td>\n",
       "      <td>The CSP’s subject matter experts shall review ...</td>\n",
       "      <td>Medina Metrics</td>\n",
       "      <td>Old EUCS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>179 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     anchor_id                                        anchor_text  target_id  \\\n",
       "0            1  1: This metric is used to assess if the antima...  OPS-05.3H   \n",
       "1            2  2: This metric is used to assess if the antima...  OPS-05.3H   \n",
       "2            3  3: This metric is used to assess if backups ar...  OPS-07.2H   \n",
       "3            3  3: This metric is used to assess if backups ar...  OPS-09.2H   \n",
       "4            4  4: This metric is used to assess the configure...  OPS-07.2H   \n",
       "..         ...                                                ...        ...   \n",
       "174        163  163: Where is access control monitored and reg...   PS-02.8H   \n",
       "175        164  164: Which tool is used for monitoring the lis...  ISP-03.5H   \n",
       "176        165  165: Which processes are in place for event de...  OPS-12.2H   \n",
       "177        166  166: How are changes in role and right managem...  CCM-05.1H   \n",
       "178        167  167: This metric checks whether the policy doc...  ISP-02.4H   \n",
       "\n",
       "                                           target_text          source  \\\n",
       "0    The CSP shall automatically monitor the system...  Medina Metrics   \n",
       "1    The CSP shall automatically monitor the system...  Medina Metrics   \n",
       "2    In order to check the proper application of th...  Medina Metrics   \n",
       "3    When the backup data is transmitted to a remot...  Medina Metrics   \n",
       "4    In order to check the proper application of th...  Medina Metrics   \n",
       "..                                                 ...             ...   \n",
       "174  The access control policy shall include loggin...  Medina Metrics   \n",
       "175  The list of exceptions shall be automatically ...  Medina Metrics   \n",
       "176  The CSP shall automatically monitor that event...  Medina Metrics   \n",
       "177  The CSP shall define roles and rights accordin...  Medina Metrics   \n",
       "178  The CSP’s subject matter experts shall review ...  Medina Metrics   \n",
       "\n",
       "       target  \n",
       "0    Old EUCS  \n",
       "1    Old EUCS  \n",
       "2    Old EUCS  \n",
       "3    Old EUCS  \n",
       "4    Old EUCS  \n",
       "..        ...  \n",
       "174  Old EUCS  \n",
       "175  Old EUCS  \n",
       "176  Old EUCS  \n",
       "177  Old EUCS  \n",
       "178  Old EUCS  \n",
       "\n",
       "[179 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ab5362a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anchor_id</th>\n",
       "      <th>anchor_text</th>\n",
       "      <th>target_id</th>\n",
       "      <th>target_text</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OPS-05.3H</td>\n",
       "      <td>The CSP shall automatically monitor the system...</td>\n",
       "      <td>ops-05</td>\n",
       "      <td>PROTECTION AGAINST MALWARE – IMPLEMENTATION: M...</td>\n",
       "      <td>Old EUCS</td>\n",
       "      <td>BSI C5 (Inferred)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ops-05.3h</td>\n",
       "      <td>The CSP shall automatically monitor the system...</td>\n",
       "      <td>ops-05</td>\n",
       "      <td>PROTECTION AGAINST MALWARE – IMPLEMENTATION: M...</td>\n",
       "      <td>Old EUCS</td>\n",
       "      <td>BSI C5 (Inferred)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OPS-07.2H</td>\n",
       "      <td>In order to check the proper application of th...</td>\n",
       "      <td>ops-07</td>\n",
       "      <td>DATA BACKUP AND RECOVERY – MONITORING: The pro...</td>\n",
       "      <td>Old EUCS</td>\n",
       "      <td>BSI C5 (Inferred)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ops-07.2h</td>\n",
       "      <td>In order to check the proper application of th...</td>\n",
       "      <td>ops-07</td>\n",
       "      <td>DATA BACKUP AND RECOVERY – MONITORING: The pro...</td>\n",
       "      <td>Old EUCS</td>\n",
       "      <td>BSI C5 (Inferred)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OPS-13.1H</td>\n",
       "      <td>The CSP shall store all log data in an integri...</td>\n",
       "      <td>ops-13</td>\n",
       "      <td>LOGGING AND MONITORING – ACCESS, STORAGE AND D...</td>\n",
       "      <td>Old EUCS</td>\n",
       "      <td>BSI C5 (Inferred)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>ops-12.2h</td>\n",
       "      <td>The CSP shall automatically monitor that event...</td>\n",
       "      <td>ops-12</td>\n",
       "      <td>LOGGING AND MONITORING – IDENTIFICATION OF EVE...</td>\n",
       "      <td>Old EUCS</td>\n",
       "      <td>BSI C5 (Inferred)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>CCM-05.1H</td>\n",
       "      <td>The CSP shall define roles and rights accordin...</td>\n",
       "      <td>ccm-05</td>\n",
       "      <td>PERFORMING AND LOGGING CHANGES: Changes to the...</td>\n",
       "      <td>Old EUCS</td>\n",
       "      <td>BSI C5 (Inferred)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>ccm-05.1h</td>\n",
       "      <td>The CSP shall define roles and rights accordin...</td>\n",
       "      <td>ccm-05</td>\n",
       "      <td>PERFORMING AND LOGGING CHANGES: Changes to the...</td>\n",
       "      <td>Old EUCS</td>\n",
       "      <td>BSI C5 (Inferred)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>ISP-02.4H</td>\n",
       "      <td>The CSP’s subject matter experts shall review ...</td>\n",
       "      <td>isp-02</td>\n",
       "      <td>SECURITY POLICIES AND PROCEDURES: Policies and...</td>\n",
       "      <td>Old EUCS</td>\n",
       "      <td>BSI C5 (Inferred)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>isp-02.4h</td>\n",
       "      <td>The CSP’s subject matter experts shall review ...</td>\n",
       "      <td>isp-02</td>\n",
       "      <td>SECURITY POLICIES AND PROCEDURES: Policies and...</td>\n",
       "      <td>Old EUCS</td>\n",
       "      <td>BSI C5 (Inferred)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     anchor_id                                        anchor_text target_id  \\\n",
       "0    OPS-05.3H  The CSP shall automatically monitor the system...    ops-05   \n",
       "1    ops-05.3h  The CSP shall automatically monitor the system...    ops-05   \n",
       "2    OPS-07.2H  In order to check the proper application of th...    ops-07   \n",
       "3    ops-07.2h  In order to check the proper application of th...    ops-07   \n",
       "4    OPS-13.1H  The CSP shall store all log data in an integri...    ops-13   \n",
       "..         ...                                                ...       ...   \n",
       "135  ops-12.2h  The CSP shall automatically monitor that event...    ops-12   \n",
       "136  CCM-05.1H  The CSP shall define roles and rights accordin...    ccm-05   \n",
       "137  ccm-05.1h  The CSP shall define roles and rights accordin...    ccm-05   \n",
       "138  ISP-02.4H  The CSP’s subject matter experts shall review ...    isp-02   \n",
       "139  isp-02.4h  The CSP’s subject matter experts shall review ...    isp-02   \n",
       "\n",
       "                                           target_text    source  \\\n",
       "0    PROTECTION AGAINST MALWARE – IMPLEMENTATION: M...  Old EUCS   \n",
       "1    PROTECTION AGAINST MALWARE – IMPLEMENTATION: M...  Old EUCS   \n",
       "2    DATA BACKUP AND RECOVERY – MONITORING: The pro...  Old EUCS   \n",
       "3    DATA BACKUP AND RECOVERY – MONITORING: The pro...  Old EUCS   \n",
       "4    LOGGING AND MONITORING – ACCESS, STORAGE AND D...  Old EUCS   \n",
       "..                                                 ...       ...   \n",
       "135  LOGGING AND MONITORING – IDENTIFICATION OF EVE...  Old EUCS   \n",
       "136  PERFORMING AND LOGGING CHANGES: Changes to the...  Old EUCS   \n",
       "137  PERFORMING AND LOGGING CHANGES: Changes to the...  Old EUCS   \n",
       "138  SECURITY POLICIES AND PROCEDURES: Policies and...  Old EUCS   \n",
       "139  SECURITY POLICIES AND PROCEDURES: Policies and...  Old EUCS   \n",
       "\n",
       "                target  \n",
       "0    BSI C5 (Inferred)  \n",
       "1    BSI C5 (Inferred)  \n",
       "2    BSI C5 (Inferred)  \n",
       "3    BSI C5 (Inferred)  \n",
       "4    BSI C5 (Inferred)  \n",
       "..                 ...  \n",
       "135  BSI C5 (Inferred)  \n",
       "136  BSI C5 (Inferred)  \n",
       "137  BSI C5 (Inferred)  \n",
       "138  BSI C5 (Inferred)  \n",
       "139  BSI C5 (Inferred)  \n",
       "\n",
       "[140 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f78375e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "GRAFICO GENERATO CORRETTAMENTE\n",
      "------------------------------------------------------------\n",
      "File salvato in: PaperPlots/train_composition_latex.pdf\n",
      "\n",
      "RIEPILOGO DATI PLOTTATI (Textual View):\n",
      "------------------------------------------------------------\n",
      "TARGET STANDARD                | SOURCE BREAKDOWN                         |  TOTAL\n",
      "----------------------------------------------------------------------------------\n",
      "EUCS                           | Cisco: 1270                              |   1270\n",
      "BSI C5                         | Cisco: 555, Fabasoft Metrics: 27, NewEUCS (Filtered): 569 |   1151\n",
      "SecNumCloud (EN)               | Cisco: 563                               |    563\n",
      "Spanish ENS (EN)               | Cisco: 318                               |    318\n",
      "EUCS (via BSI C5)              | Fabasoft Metrics: 206                    |    206\n",
      "------------------------------------------------------------\n",
      "Nota: Il grafico PDF vettoriale include pattern e font LaTeX.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# 1. CONFIGURAZIONE BACKEND LATEX\n",
    "# Nota: Questo backend genera file PDF di alta qualità ma non mostra finestre a schermo.\n",
    "matplotlib.use(\"pgf\")\n",
    "plt.rcParams.update({\n",
    "    \"pgf.texsystem\": \"pdflatex\",\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [],\n",
    "    \"axes.labelsize\": 16,\n",
    "    \"font.size\": 14,\n",
    "    \"legend.fontsize\": 12,\n",
    "    \"xtick.labelsize\": 12,\n",
    "    \"ytick.labelsize\": 12,\n",
    "    \"pgf.rcfonts\": False,\n",
    "})\n",
    "\n",
    "# 2. PREPARAZIONE DATI\n",
    "# Assumiamo che 'train' sia già caricato nel notebook\n",
    "if 'train' not in locals():\n",
    "    print(\"ERRORE: Il DataFrame 'train' non è definito.\")\n",
    "else:\n",
    "    # Creiamo la tabella incrociata\n",
    "    df_plot = pd.crosstab(train['target'], train['source'])\n",
    "    # Ordiniamo per totale decrescente\n",
    "    df_plot = df_plot.loc[df_plot.sum(axis=1).sort_values(ascending=False).index]\n",
    "\n",
    "    # 3. DEFINIZIONE COLORI RICHIESTI\n",
    "    custom_colors = ['tab:blue', 'tab:orange', 'tab:green', 'gold']\n",
    "\n",
    "    # 4. CREAZIONE GRAFICO (Oggetto Figure)\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    df_plot.plot(\n",
    "        kind='bar', \n",
    "        stacked=True, \n",
    "        color=custom_colors, \n",
    "        edgecolor='black', \n",
    "        linewidth=0.8,\n",
    "        width=0.6,\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "    # 5. AGGIUNTA PATTERN\n",
    "    patterns = ['//', '\\\\\\\\', 'xx', '..']\n",
    "    for i, container in enumerate(ax.containers):\n",
    "        hatch = patterns[i % len(patterns)]\n",
    "        for patch in container:\n",
    "            patch.set_hatch(hatch)\n",
    "\n",
    "    # 6. ETICHETTE TOTALI\n",
    "    totals = df_plot.sum(axis=1)\n",
    "    for i, total in enumerate(totals):\n",
    "        ax.text(\n",
    "            i, \n",
    "            total + (total * 0.02), \n",
    "            int(total), \n",
    "            ha='center', \n",
    "            va='bottom', \n",
    "            fontsize=12, \n",
    "            color='black'\n",
    "        )\n",
    "\n",
    "    # 7. FORMATTAZIONE\n",
    "    ax.set_title(\"\") \n",
    "    ax.set_xlabel(\"Target Standard (Positive)\", fontsize=16, labelpad=10)\n",
    "    ax.set_ylabel(\"Number of Semantic Pairs\", fontsize=16, labelpad=10)\n",
    "    plt.xticks(rotation=0)\n",
    "\n",
    "    ax.legend(\n",
    "        title=\"Source (Anchor)\", \n",
    "        title_fontsize=12, \n",
    "        loc='upper right', \n",
    "        frameon=True, \n",
    "        framealpha=0.95\n",
    "    )\n",
    "\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    # 8. SALVATAGGIO PDF\n",
    "    output_path = 'PaperPlots/train_composition_latex.pdf'\n",
    "    # Assicuriamoci che la cartella esista (opzionale, ma consigliato)\n",
    "    import os\n",
    "    os.makedirs('PaperPlots', exist_ok=True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, bbox_inches='tight')\n",
    "    plt.close(fig) # Chiude la figura per liberare memoria\n",
    "\n",
    "    # ==============================================================================\n",
    "    # 9. OUTPUT TESTUALE NEL NOTEBOOK (DATA SUMMARY)\n",
    "    # ==============================================================================\n",
    "    print(\"-\" * 60)\n",
    "    print(\"GRAFICO GENERATO CORRETTAMENTE\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"File salvato in: {output_path}\")\n",
    "    print(\"\\nRIEPILOGO DATI PLOTTATI (Textual View):\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Calcoliamo i totali per colonna per visualizzarli\n",
    "    df_summary = df_plot.copy()\n",
    "    df_summary['TOTAL'] = df_summary.sum(axis=1)\n",
    "    \n",
    "    # Formattiamo l'output come una tabella Markdown-like\n",
    "    header = f\"{'TARGET STANDARD':<30} | {'SOURCE BREAKDOWN':<40} | {'TOTAL':>6}\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "    \n",
    "    for target_name, row in df_summary.iterrows():\n",
    "        total = row['TOTAL']\n",
    "        # Costruiamo una stringa che dice es: \"Cisco (300), Fabasoft (50)\"\n",
    "        breakdown_parts = []\n",
    "        for source_col in df_plot.columns:\n",
    "            val = row[source_col]\n",
    "            if val > 0:\n",
    "                breakdown_parts.append(f\"{source_col}: {val}\")\n",
    "        \n",
    "        breakdown_str = \", \".join(breakdown_parts)\n",
    "        \n",
    "        # Stampa riga formattata\n",
    "        print(f\"{target_name:<30} | {breakdown_str:<40} | {total:>6}\")\n",
    "        \n",
    "    print(\"-\" * 60)\n",
    "    print(\"Nota: Il grafico PDF vettoriale include pattern e font LaTeX.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25e7c296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "GRAFICO GENERATO: Visualizzazione Semantica dei Test Set\n",
      "------------------------------------------------------------\n",
      "File salvato in: PaperPlots/test_composition_visual_latex.pdf\n",
      "\n",
      "Legenda Visuale Applicata:\n",
      "1. Test Set A (Benchmark):\n",
      "   - COLORE VIOLA (Sorgente)  = Medina Metrics\n",
      "   - PATTERN RIGHE (Target)   = Old EUCS\n",
      "\n",
      "2. Test Set B (Generalization):\n",
      "   - COLORE BLU (Sorgente)    = Old EUCS Reqs\n",
      "   - PATTERN PUNTINI (Target) = BSI C5\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# 1. CONFIGURAZIONE LATEX\n",
    "matplotlib.use(\"pgf\")\n",
    "plt.rcParams.update({\n",
    "    \"pgf.texsystem\": \"pdflatex\",\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [],\n",
    "    \"axes.labelsize\": 16,\n",
    "    \"font.size\": 14,\n",
    "    \"legend.fontsize\": 12,\n",
    "    \"xtick.labelsize\": 13,\n",
    "    \"ytick.labelsize\": 13,\n",
    "    \"pgf.rcfonts\": False,\n",
    "})\n",
    "\n",
    "# 2. DATI\n",
    "# Test Set A: Medina (Source) -> Old EUCS (Target) -> 179 items\n",
    "# Test Set B: Old EUCS (Source) -> BSI C5 (Target) -> 140 items\n",
    "sets = ['Test Set A\\n(Benchmark)', 'Test Set B\\n(Generalization)']\n",
    "counts = [179, 140]\n",
    "\n",
    "# 3. DEFINIZIONE STILE VISIVO (La \"Grammatica\" del grafico)\n",
    "# Colore = SORGENTE\n",
    "color_map = {\n",
    "    'Medina Metrics': 'tab:purple',\n",
    "    'Old EUCS': 'tab:blue'\n",
    "}\n",
    "# Pattern = TARGET\n",
    "hatch_map = {\n",
    "    'Old EUCS': '///',   # Righe diagonali\n",
    "    'BSI C5': '...'      # Puntini\n",
    "}\n",
    "\n",
    "# Associazione logica per ogni barra\n",
    "bar_configs = [\n",
    "    {'source': 'Medina Metrics', 'target': 'Old EUCS', 'count': 179},\n",
    "    {'source': 'Old EUCS',       'target': 'BSI C5',   'count': 140}\n",
    "]\n",
    "\n",
    "# 4. CREAZIONE GRAFICO\n",
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "\n",
    "# Disegniamo le barre una per una per controllarne colore e pattern\n",
    "bars = []\n",
    "for i, config in enumerate(bar_configs):\n",
    "    color = color_map[config['source']]\n",
    "    hatch = hatch_map[config['target']]\n",
    "    \n",
    "    # Disegna la barra\n",
    "    bar = ax.bar(\n",
    "        i, \n",
    "        config['count'], \n",
    "        color=color, \n",
    "        edgecolor='black', \n",
    "        hatch=hatch,     # <--- Qui sta la magia visiva\n",
    "        width=0.5,\n",
    "        linewidth=1.0,\n",
    "        alpha=0.9\n",
    "    )\n",
    "    \n",
    "    # Etichetta col numero sopra\n",
    "    ax.text(\n",
    "        i, \n",
    "        config['count'] + 5, \n",
    "        str(config['count']), \n",
    "        ha='center', \n",
    "        va='bottom', \n",
    "        fontweight='bold', \n",
    "        fontsize=14\n",
    "    )\n",
    "\n",
    "# 5. CREAZIONE LEGENDA SEMANTICA (Source vs Target)\n",
    "# Creiamo due gruppi di legende per spiegare la visual grammar\n",
    "\n",
    "# Gruppo 1: Sorgenti (Colori)\n",
    "legend_handles = []\n",
    "legend_handles.append(mpatches.Patch(color='none', label=r\"\\textbf{SOURCE DATA (Color):}\")) # Titolo fittizio\n",
    "legend_handles.append(mpatches.Patch(facecolor='tab:purple', edgecolor='black', label='Medina Metrics'))\n",
    "legend_handles.append(mpatches.Patch(facecolor='tab:blue', edgecolor='black', label='Old EUCS Reqs'))\n",
    "\n",
    "# Spazio vuoto\n",
    "legend_handles.append(mpatches.Patch(color='none', label='')) \n",
    "\n",
    "# Gruppo 2: Target (Pattern)\n",
    "legend_handles.append(mpatches.Patch(color='none', label=r\"\\textbf{TARGET STANDARD (Pattern):}\")) # Titolo fittizio\n",
    "# Per mostrare solo il pattern nella legenda usiamo facecolor='white' e hatch nero\n",
    "legend_handles.append(mpatches.Patch(facecolor='white', edgecolor='black', hatch='///', label='Old EUCS'))\n",
    "legend_handles.append(mpatches.Patch(facecolor='white', edgecolor='black', hatch='...', label='BSI C5'))\n",
    "\n",
    "# Posizionamento Legenda\n",
    "ax.legend(\n",
    "    handles=legend_handles, \n",
    "    loc='upper right', \n",
    "    bbox_to_anchor=(1.45, 1.02), # Fuori dal grafico a destra\n",
    "    frameon=True,\n",
    "    fontsize=11\n",
    ")\n",
    "\n",
    "# 6. FORMATTAZIONE ASSI\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_xticklabels(sets)\n",
    "ax.set_ylabel(\"Number of Evaluation Pairs\", fontsize=16, labelpad=10)\n",
    "ax.set_xlabel(\"Evaluation Scenario\", fontsize=16, labelpad=10)\n",
    "ax.set_ylim(0, 200) # Un po' di aria sopra\n",
    "\n",
    "# Rimuovi bordi inutili\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# 7. SALVATAGGIO\n",
    "plt.tight_layout()\n",
    "output_path = 'PaperPlots/test_composition_visual_latex.pdf'\n",
    "plt.savefig(output_path, bbox_inches='tight')\n",
    "\n",
    "# Output testuale per il notebook\n",
    "print(\"-\" * 60)\n",
    "print(\"GRAFICO GENERATO: Visualizzazione Semantica dei Test Set\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"File salvato in: {output_path}\")\n",
    "print(\"\\nLegenda Visuale Applicata:\")\n",
    "print(\"1. Test Set A (Benchmark):\")\n",
    "print(\"   - COLORE VIOLA (Sorgente)  = Medina Metrics\")\n",
    "print(\"   - PATTERN RIGHE (Target)   = Old EUCS\")\n",
    "print(\"\\n2. Test Set B (Generalization):\")\n",
    "print(\"   - COLORE BLU (Sorgente)    = Old EUCS Reqs\")\n",
    "print(\"   - PATTERN PUNTINI (Target) = BSI C5\")\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8a7e06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
