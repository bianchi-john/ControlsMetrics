{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "591027f2",
   "metadata": {},
   "source": [
    "# Generate training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e0951c",
   "metadata": {},
   "source": [
    "### CISCO and Spaninsh Ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d8a9d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Caricamento file...\n",
      "   - Cisco loaded: (713, 32) righe\n",
      "   - Spanish ENS loaded: (213, 8) righe\n",
      "\n",
      "2. Indicizzazione e Aggregazione ENS (English)...\n",
      "   - Indicizzati 169 codici ENS univoci con testo in Inglese.\n",
      "\n",
      "3. Generazione coppie di training (Cisco -> ENS)...\n",
      "------------------------------\n",
      "RISULTATO FINALE\n",
      "------------------------------\n",
      "Totale coppie generate: 318\n",
      "Codici mancanti (nel file ENS): 28\n",
      "Esempio mancanti: ['mp.info.2', 'mp.com.9', 'mp.s.1', 'mp.eq.2', 'mp.info.9']\n",
      "\n",
      "File salvato come: TrainAndTestData/trainingData/training_data_step1_cisco_ens.csv\n",
      "\n",
      "Anteprima dati:\n",
      "                                         anchor_text  \\\n",
      "0  Control Self-Assessments: Independent Control ...   \n",
      "1  Control Self-Assessments: Independent Control ...   \n",
      "\n",
      "                                         target_text  \n",
      "0  Metrics system. Taking into account the securi...  \n",
      "1  Certified components. Is the Catalog of Inform...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURAZIONE\n",
    "# ==============================================================================\n",
    "CISCO_PATH = \"Schemes/Cisco.csv\"         # Assicurati che il percorso sia corretto\n",
    "ENS_PATH = \"Schemes/SpanishENS.csv\"      # Assicurati che il percorso sia corretto\n",
    "OUTPUT_FILE = \"TrainAndTestData/trainingData/training_data_step1_cisco_ens.csv\"\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CARICAMENTO DATI\n",
    "# ==============================================================================\n",
    "print(\"1. Caricamento file...\")\n",
    "\n",
    "# Caricamento Cisco\n",
    "df_cisco = pd.read_csv(CISCO_PATH)\n",
    "print(f\"   - Cisco loaded: {df_cisco.shape} righe\")\n",
    "\n",
    "# Caricamento Spanish ENS (con separatore punto e virgola)\n",
    "# Usiamo dtype=str per evitare problemi con codici interpretati come numeri\n",
    "df_ens = pd.read_csv(ENS_PATH, sep=';', dtype=str)\n",
    "print(f\"   - Spanish ENS loaded: {df_ens.shape} righe\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. CREAZIONE LOOKUP TABLE (ENS INGLESE AGGREGATO)\n",
    "# ==============================================================================\n",
    "print(\"\\n2. Indicizzazione e Aggregazione ENS (English)...\")\n",
    "\n",
    "ens_lookup = {}\n",
    "\n",
    "# Raggruppiamo per codice perché nel CSV lo stesso codice può apparire più volte\n",
    "# (es. una riga per il titolo, una riga per le domande)\n",
    "grouped_ens = df_ens.groupby('Codice')\n",
    "\n",
    "for code, group in grouped_ens:\n",
    "    # Normalizza la chiave (codice)\n",
    "    if pd.isna(code): continue\n",
    "    clean_code = str(code).strip().lower()\n",
    "    \n",
    "    # Raccogli tutti i testi INGLESI disponibili per questo codice\n",
    "    texts = []\n",
    "    \n",
    "    for _, row in group.iterrows():\n",
    "        # Titolo Inglese\n",
    "        if 'Titolo_EN' in row and not pd.isna(row['Titolo_EN']):\n",
    "            t = str(row['Titolo_EN']).strip()\n",
    "            if t: texts.append(t)\n",
    "            \n",
    "        # Domande Inglese\n",
    "        if 'Domande_EN' in row and not pd.isna(row['Domande_EN']):\n",
    "            q = str(row['Domande_EN']).strip()\n",
    "            # Rimuovi i pipe '|' che separano le domande e sostituisci con spazio\n",
    "            q = q.replace('|', ' ')\n",
    "            if q: texts.append(q)\n",
    "            \n",
    "    # Unisci tutto il testo raccolto per questo controllo\n",
    "    full_text_en = \". \".join(texts)\n",
    "    \n",
    "    # Pulizia finale (spazi doppi, ecc)\n",
    "    full_text_en = re.sub(r'\\s+', ' ', full_text_en).strip()\n",
    "    \n",
    "    if full_text_en:\n",
    "        ens_lookup[clean_code] = full_text_en\n",
    "\n",
    "print(f\"   - Indicizzati {len(ens_lookup)} codici ENS univoci con testo in Inglese.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. MAPPING E GENERAZIONE DATASET\n",
    "# ==============================================================================\n",
    "print(\"\\n3. Generazione coppie di training (Cisco -> ENS)...\")\n",
    "\n",
    "training_data = []\n",
    "stats = {\n",
    "    \"matches\": 0,\n",
    "    \"missing\": set()\n",
    "}\n",
    "\n",
    "# Colonne Cisco dove cercare i riferimenti ENS\n",
    "ens_columns = ['Spanish ENS BASIC Control', 'Spanish ENS Medium Control', 'Spanish ENS High Control']\n",
    "\n",
    "for idx, row in df_cisco.iterrows():\n",
    "    # Anchor (Cisco)\n",
    "    # Combiniamo Titolo e Wording per la massima ricchezza semantica\n",
    "    if pd.isna(row['Control Reference']): continue\n",
    "    \n",
    "    anchor_id = str(row['Control Reference'])\n",
    "    title = str(row['Control Title']) if not pd.isna(row['Control Title']) else \"\"\n",
    "    wording = str(row['Control Wording']) if not pd.isna(row['Control Wording']) else \"\"\n",
    "    \n",
    "    anchor_text = f\"{title}: {wording}\".strip()\n",
    "    \n",
    "    # Target (ENS)\n",
    "    # Cerchiamo i codici nelle 3 colonne\n",
    "    target_codes = []\n",
    "    for col in ens_columns:\n",
    "        if col in row and not pd.isna(row[col]):\n",
    "            # Split per virgola in caso ci siano più controlli in una cella\n",
    "            codes_in_cell = str(row[col]).split(',')\n",
    "            target_codes.extend([c.strip().lower() for c in codes_in_cell])\n",
    "    \n",
    "    # Rimuovi duplicati e itera\n",
    "    unique_targets = set(target_codes)\n",
    "    \n",
    "    for t_code in unique_targets:\n",
    "        if not t_code: continue\n",
    "        \n",
    "        if t_code in ens_lookup:\n",
    "            # MATCH TROVATO\n",
    "            positive_text = ens_lookup[t_code]\n",
    "            \n",
    "            training_data.append({\n",
    "                'anchor_id': anchor_id,\n",
    "                'anchor_text': anchor_text,\n",
    "                'target_id': t_code,\n",
    "                'target_text': positive_text,\n",
    "                'source': 'Cisco',\n",
    "                'target': 'Spanish ENS (EN)'\n",
    "            })\n",
    "            stats[\"matches\"] += 1\n",
    "        else:\n",
    "            stats[\"missing\"].add(t_code)\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. SALVATAGGIO\n",
    "# ==============================================================================\n",
    "df_train = pd.DataFrame(training_data)\n",
    "df_train.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(\"RISULTATO FINALE\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Totale coppie generate: {len(df_train)}\")\n",
    "print(f\"Codici mancanti (nel file ENS): {len(stats['missing'])}\")\n",
    "if stats['missing']:\n",
    "    print(f\"Esempio mancanti: {list(stats['missing'])[:5]}\")\n",
    "print(f\"\\nFile salvato come: {OUTPUT_FILE}\")\n",
    "\n",
    "# Anteprima\n",
    "print(\"\\nAnteprima dati:\")\n",
    "print(df_train[['anchor_text', 'target_text']].head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cce0c1",
   "metadata": {},
   "source": [
    "### CISCO and Spaninsh Ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e03cbb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Caricamento e indicizzazione SecNumCloud...\n",
      "   - SecNumCloud caricato: 287 righe.\n",
      "   - Indicizzati 261 controlli SecNumCloud univoci (EN).\n",
      "\n",
      "2. Generazione coppie di training (Cisco -> SecNumCloud)...\n",
      "   - Cisco caricato: 713 righe.\n",
      "------------------------------\n",
      "RISULTATO STEP 2\n",
      "------------------------------\n",
      "Nuove coppie generate: 567\n",
      "Codici SecNumCloud non trovati: 33\n",
      "Esempio mancanti: ['18.2.1.b', '9.6.k', '9.6.g']...\n",
      "\n",
      "File salvato: TrainAndTestData/trainingData/training_data_step2_cisco_secNumCloud.csv\n",
      "\n",
      "Anteprima dati:\n",
      "  anchor_id target_id                                        target_text\n",
      "0     CCF 1  18.2.1.a  The service provider must document and impleme...\n",
      "1     CCF 1    18.4.a  The service provider must document and impleme...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURAZIONE PERCORSI\n",
    "# ==============================================================================\n",
    "# Percorsi basati sulla tua struttura di cartelle\n",
    "PATH_CISCO = \"Schemes/Cisco.csv\"\n",
    "PATH_SECNUM = \"Schemes/Secnumcloud.csv\"\n",
    "OUTPUT_FILE = \"TrainAndTestData/trainingData/training_data_step2_cisco_secNumCloud.csv\"\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CARICAMENTO E INDICIZZAZIONE SECNUMCLOUD\n",
    "# ==============================================================================\n",
    "print(\"1. Caricamento e indicizzazione SecNumCloud...\")\n",
    "\n",
    "try:\n",
    "    # Tentativo di lettura standard (virgola)\n",
    "    df_sec = pd.read_csv(PATH_SECNUM)\n",
    "    \n",
    "    # Verifica colonne critiche\n",
    "    if 'ID' not in df_sec.columns or 'Description_EN' not in df_sec.columns:\n",
    "        # Fallback: prova con punto e virgola se la prima lettura non ha parsato le colonne\n",
    "        df_sec = pd.read_csv(PATH_SECNUM, sep=';')\n",
    "\n",
    "    print(f\"   - SecNumCloud caricato: {len(df_sec)} righe.\")\n",
    "except Exception as e:\n",
    "    print(f\"   - ERRORE critico nel caricamento di SecNumCloud: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Creazione dizionario di lookup: ID (lowercase) -> Descrizione Inglese\n",
    "secnum_lookup = {}\n",
    "for _, row in df_sec.iterrows():\n",
    "    if pd.isna(row['ID']): continue\n",
    "    \n",
    "    # Normalizzazione ID (es: \"5.1.A \" -> \"5.1.a\")\n",
    "    clean_id = str(row['ID']).strip().lower()\n",
    "    \n",
    "    # Estrazione testo inglese\n",
    "    if 'Description_EN' in row and not pd.isna(row['Description_EN']):\n",
    "        desc = str(row['Description_EN']).strip()\n",
    "        secnum_lookup[clean_id] = desc\n",
    "\n",
    "print(f\"   - Indicizzati {len(secnum_lookup)} controlli SecNumCloud univoci (EN).\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. MAPPING CISCO -> SECNUMCLOUD\n",
    "# ==============================================================================\n",
    "print(\"\\n2. Generazione coppie di training (Cisco -> SecNumCloud)...\")\n",
    "\n",
    "df_cisco = pd.read_csv(PATH_CISCO)\n",
    "print(f\"   - Cisco caricato: {len(df_cisco)} righe.\")\n",
    "\n",
    "training_data = []\n",
    "stats = {\n",
    "    \"matches\": 0,\n",
    "    \"missing\": set()\n",
    "}\n",
    "\n",
    "# La colonna Cisco che contiene i riferimenti\n",
    "target_col = 'SecNumCloud Control'\n",
    "\n",
    "for idx, row in df_cisco.iterrows():\n",
    "    # Verifica se c'è un mapping\n",
    "    if target_col not in row or pd.isna(row[target_col]):\n",
    "        continue\n",
    "    \n",
    "    # 1. Preparazione Anchor (Cisco)\n",
    "    anchor_id = str(row['Control Reference'])\n",
    "    title = str(row['Control Title']) if not pd.isna(row['Control Title']) else \"\"\n",
    "    wording = str(row['Control Wording']) if not pd.isna(row['Control Wording']) else \"\"\n",
    "    \n",
    "    # Uniamo titolo e wording per dare contesto completo al modello\n",
    "    anchor_text = f\"{title}: {wording}\".strip()\n",
    "    \n",
    "    # 2. Preparazione Target (SecNumCloud IDs)\n",
    "    # Cisco può contenere liste tipo \"5.1.a, 5.2.b\" o usare newline\n",
    "    raw_refs = str(row[target_col]).replace('\\n', ',')\n",
    "    target_ids_list = [t.strip().lower() for t in raw_refs.split(',')]\n",
    "    \n",
    "    # 3. Matching\n",
    "    for t_id in set(target_ids_list): # set() per evitare duplicati sulla stessa riga\n",
    "        if not t_id: continue\n",
    "        \n",
    "        if t_id in secnum_lookup:\n",
    "            # MATCH TROVATO\n",
    "            positive_text = secnum_lookup[t_id]\n",
    "            \n",
    "            training_data.append({\n",
    "                'anchor_id': anchor_id,\n",
    "                'anchor_text': anchor_text,\n",
    "                'target_id': t_id,      # ID normalizzato\n",
    "                'target_text': positive_text,\n",
    "                'source': 'Cisco',\n",
    "                'target': 'SecNumCloud (EN)'\n",
    "            })\n",
    "            stats[\"matches\"] += 1\n",
    "        else:\n",
    "            # MATCH NON TROVATO (ID presente in Cisco ma non nel file SecNumCloud)\n",
    "            stats[\"missing\"].add(t_id)\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. SALVATAGGIO FILE STEP 2\n",
    "# ==============================================================================\n",
    "df_result = pd.DataFrame(training_data)\n",
    "df_result.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(\"RISULTATO STEP 2\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Nuove coppie generate: {len(df_result)}\")\n",
    "print(f\"Codici SecNumCloud non trovati: {len(stats['missing'])}\")\n",
    "if stats['missing']:\n",
    "    print(f\"Esempio mancanti: {list(stats['missing'])[:3]}...\")\n",
    "print(f\"\\nFile salvato: {OUTPUT_FILE}\")\n",
    "\n",
    "# Anteprima\n",
    "print(\"\\nAnteprima dati:\")\n",
    "print(df_result[['anchor_id', 'target_id', 'target_text']].head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39a25b2",
   "metadata": {},
   "source": [
    "### Cisco and BSIC5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c24fe34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Caricamento e indicizzazione BSI C5...\n",
      "   - BSI C5 JSON caricato. Totale elementi: 294\n",
      "   - Indicizzati 223 controlli BSI con testo.\n",
      "\n",
      "2. Generazione coppie di training (Cisco -> BSI C5)...\n",
      "   - Cisco CSV caricato: 713 righe.\n",
      "------------------------------\n",
      "RISULTATO STEP 3\n",
      "------------------------------\n",
      "Nuove coppie generate: 555\n",
      "Codici BSI citati in Cisco ma non trovati nel JSON: 0\n",
      "\n",
      "File salvato in: TrainAndTestData/trainingData/training_data_step3_cisco_bsi.csv\n",
      "\n",
      "Anteprima dati:\n",
      "  anchor_id target_id                                        target_text\n",
      "0     CCF 1    ois-01  Information Security Management System (ISMS):...\n",
      "1     CCF 1     sp-02  Review and Approval of Policies and Instructio...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURAZIONE PERCORSI\n",
    "# ==============================================================================\n",
    "PATH_CISCO = \"Schemes/Cisco.csv\"\n",
    "PATH_BSI = \"Schemes/BSI-C5.json\"\n",
    "OUTPUT_FILE = \"TrainAndTestData/trainingData/training_data_step3_cisco_bsi.csv\"\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CARICAMENTO E INDICIZZAZIONE BSI C5 (JSON)\n",
    "# ==============================================================================\n",
    "print(\"1. Caricamento e indicizzazione BSI C5...\")\n",
    "\n",
    "try:\n",
    "    with open(PATH_BSI, 'r', encoding='utf-8') as f:\n",
    "        bsi_data = json.load(f)\n",
    "    print(f\"   - BSI C5 JSON caricato. Totale elementi: {len(bsi_data)}\")\n",
    "except Exception as e:\n",
    "    print(f\"   - ERRORE CRITICO nel caricamento del JSON BSI: {e}\")\n",
    "    # In un notebook, fermiamo l'esecuzione qui se il file non c'è\n",
    "    raise e\n",
    "\n",
    "bsi_lookup = {}\n",
    "\n",
    "for item in bsi_data:\n",
    "    # Verifica che il codice esista\n",
    "    if 'code' not in item or not item['code']:\n",
    "        continue\n",
    "        \n",
    "    # Normalizzazione ID (es. \"IAM-02\" -> \"iam-02\")\n",
    "    code = str(item['code']).strip().lower()\n",
    "    \n",
    "    # Costruzione del Testo Ricco\n",
    "    # Combiniamo Nome, Titolo Descrizione e Descrizione Estesa\n",
    "    name = item.get('name', '')\n",
    "    desc_title = item.get('descriptionTitle', '')\n",
    "    desc_ext = item.get('descriptionExtended', '')\n",
    "    \n",
    "    # Pulizia valori None/Null (che nel JSON appaiono come null)\n",
    "    if name is None: name = \"\"\n",
    "    if desc_title is None: desc_title = \"\"\n",
    "    if desc_ext is None: desc_ext = \"\"\n",
    "    \n",
    "    # Unione stringhe\n",
    "    full_text = f\"{name}: {desc_title} {desc_ext}\".strip()\n",
    "    \n",
    "    # Pulizia spazi extra e newline\n",
    "    full_text = re.sub(r'\\s+', ' ', full_text).strip()\n",
    "    \n",
    "    if full_text:\n",
    "        bsi_lookup[code] = full_text\n",
    "\n",
    "print(f\"   - Indicizzati {len(bsi_lookup)} controlli BSI con testo.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. MAPPING CISCO -> BSI C5\n",
    "# ==============================================================================\n",
    "print(\"\\n2. Generazione coppie di training (Cisco -> BSI C5)...\")\n",
    "\n",
    "try:\n",
    "    df_cisco = pd.read_csv(PATH_CISCO)\n",
    "    print(f\"   - Cisco CSV caricato: {len(df_cisco)} righe.\")\n",
    "except Exception as e:\n",
    "    print(f\"   - ERRORE CRITICO nel caricamento Cisco: {e}\")\n",
    "    raise e\n",
    "\n",
    "training_data = []\n",
    "stats = {\n",
    "    \"matches\": 0,\n",
    "    \"missing\": set()\n",
    "}\n",
    "\n",
    "# La colonna in Cisco si chiama esattamente \"BSI C5\"\n",
    "target_col = 'BSI C5'\n",
    "\n",
    "for idx, row in df_cisco.iterrows():\n",
    "    # Verifica se c'è un mapping in questa riga\n",
    "    if target_col not in row or pd.isna(row[target_col]):\n",
    "        continue\n",
    "        \n",
    "    # 1. Preparazione Anchor (Cisco)\n",
    "    anchor_id = str(row['Control Reference'])\n",
    "    title = str(row['Control Title']) if not pd.isna(row['Control Title']) else \"\"\n",
    "    wording = str(row['Control Wording']) if not pd.isna(row['Control Wording']) else \"\"\n",
    "    \n",
    "    anchor_text = f\"{title}: {wording}\".strip()\n",
    "    anchor_text = re.sub(r'\\s+', ' ', anchor_text) # pulizia veloce spazi\n",
    "    \n",
    "    # 2. Preparazione Target (BSI IDs)\n",
    "    # I codici in Cisco sono separati da virgola (es. \"OIS-01, SP-01\")\n",
    "    raw_refs = str(row[target_col]).replace('\\n', ',')\n",
    "    target_ids_list = [t.strip().lower() for t in raw_refs.split(',')]\n",
    "    \n",
    "    # 3. Matching\n",
    "    for t_id in set(target_ids_list): # set() per rimuovere duplicati\n",
    "        if not t_id: continue\n",
    "        \n",
    "        if t_id in bsi_lookup:\n",
    "            # MATCH TROVATO\n",
    "            positive_text = bsi_lookup[t_id]\n",
    "            \n",
    "            training_data.append({\n",
    "                'anchor_id': anchor_id,\n",
    "                'anchor_text': anchor_text,\n",
    "                'target_id': t_id,      # ID normalizzato\n",
    "                'target_text': positive_text,\n",
    "                'source': 'Cisco',\n",
    "                'target': 'BSI C5'\n",
    "            })\n",
    "            stats[\"matches\"] += 1\n",
    "        else:\n",
    "            # MATCH NON TROVATO\n",
    "            stats[\"missing\"].add(t_id)\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. SALVATAGGIO FILE STEP 3\n",
    "# ==============================================================================\n",
    "df_result = pd.DataFrame(training_data)\n",
    "df_result.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(\"RISULTATO STEP 3\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Nuove coppie generate: {len(df_result)}\")\n",
    "print(f\"Codici BSI citati in Cisco ma non trovati nel JSON: {len(stats['missing'])}\")\n",
    "if stats['missing']:\n",
    "    print(f\"Esempi codici mancanti: {list(stats['missing'])[:5]}...\")\n",
    "print(f\"\\nFile salvato in: {OUTPUT_FILE}\")\n",
    "\n",
    "# Anteprima\n",
    "print(\"\\nAnteprima dati:\")\n",
    "print(df_result[['anchor_id', 'target_id', 'target_text']].head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e376f4d",
   "metadata": {},
   "source": [
    "### Cisco and NewEUCS Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dfd76e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Caricamento e indicizzazione EUCS...\n",
      "   - EUCS caricato: 529 righe.\n",
      "   - Indicizzati 522 requisiti EUCS con testo.\n",
      "\n",
      "2. Generazione coppie di training (Cisco -> EUCS)...\n",
      "------------------------------\n",
      "RISULTATO STEP 4\n",
      "------------------------------\n",
      "Nuove coppie generate: 1272\n",
      "Codici EUCS non trovati: 14\n",
      "Esempio mancanti: ['rm-02.1b', 'cs-09.2', 'ois-01.4', 'ps-04.4a', 'cs-03.2a']...\n",
      "\n",
      "File salvato in: TrainAndTestData/trainingData/training_data_step4_cisco_eucs.csv\n",
      "\n",
      "Anteprima dati:\n",
      "  anchor_id target_id                                        target_text\n",
      "0     CCF 1   co-02.2  The CSP shall document and implement an audit ...\n",
      "1     CCF 1   co-01.2  The CSP shall document and implement procedure...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURAZIONE PERCORSI\n",
    "# ==============================================================================\n",
    "PATH_CISCO = \"Schemes/Cisco.csv\"\n",
    "PATH_EUCS = \"Schemes/NewEucsRequirements_with_texts.csv\"\n",
    "OUTPUT_FILE = \"TrainAndTestData/trainingData/training_data_step4_cisco_eucs.csv\"\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CARICAMENTO E INDICIZZAZIONE EUCS\n",
    "# ==============================================================================\n",
    "print(\"1. Caricamento e indicizzazione EUCS...\")\n",
    "\n",
    "try:\n",
    "    # Tentativo primario con virgola (basato sul tuo 'head')\n",
    "    df_eucs = pd.read_csv(PATH_EUCS, sep=',', encoding='utf-8')\n",
    "    \n",
    "    # Verifica se le colonne chiave esistono, altrimenti riprova con ;\n",
    "    if 'EUCS Ref (Detailed)' not in df_eucs.columns:\n",
    "        print(\"   - Separatore virgola fallito, provo punto e virgola...\")\n",
    "        df_eucs = pd.read_csv(PATH_EUCS, sep=';', encoding='utf-8')\n",
    "        \n",
    "    print(f\"   - EUCS caricato: {len(df_eucs)} righe.\")\n",
    "except Exception as e:\n",
    "    print(f\"   - ERRORE CRITICO caricamento EUCS: {e}\")\n",
    "    # Fallback encoding latin-1 se utf-8 fallisce\n",
    "    try:\n",
    "        df_eucs = pd.read_csv(PATH_EUCS, sep=',', encoding='latin-1')\n",
    "        print(\"   - EUCS caricato con encoding latin-1.\")\n",
    "    except:\n",
    "        raise e\n",
    "\n",
    "eucs_lookup = {}\n",
    "\n",
    "for _, row in df_eucs.iterrows():\n",
    "    # La chiave è il riferimento dettagliato (es. OIS-01.1)\n",
    "    if 'EUCS Ref (Detailed)' not in row or pd.isna(row['EUCS Ref (Detailed)']):\n",
    "        continue\n",
    "        \n",
    "    # Normalizzazione ID\n",
    "    code = str(row['EUCS Ref (Detailed)']).strip().lower()\n",
    "    \n",
    "    # Estrazione testo\n",
    "    text = \"\"\n",
    "    if 'EUCS Text' in row and not pd.isna(row['EUCS Text']):\n",
    "        text = str(row['EUCS Text']).strip()\n",
    "        # Pulizia caratteri strani (es. newline o pipe)\n",
    "        text = text.replace('\\n', ' ').replace('|', ' ')\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "    if code and text:\n",
    "        eucs_lookup[code] = text\n",
    "\n",
    "print(f\"   - Indicizzati {len(eucs_lookup)} requisiti EUCS con testo.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. MAPPING CISCO -> EUCS\n",
    "# ==============================================================================\n",
    "print(\"\\n2. Generazione coppie di training (Cisco -> EUCS)...\")\n",
    "\n",
    "df_cisco = pd.read_csv(PATH_CISCO)\n",
    "training_data = []\n",
    "stats = {\n",
    "    \"matches\": 0,\n",
    "    \"missing\": set()\n",
    "}\n",
    "\n",
    "# Colonne target in Cisco (ne abbiamo 3 per l'EUCS)\n",
    "target_columns = [\n",
    "    'EUCS Basic Control', \n",
    "    'EUCS Substantial Control', \n",
    "    'EUCS High Control'\n",
    "]\n",
    "\n",
    "for idx, row in df_cisco.iterrows():\n",
    "    # 1. Anchor (Cisco)\n",
    "    anchor_id = str(row['Control Reference'])\n",
    "    title = str(row['Control Title']) if not pd.isna(row['Control Title']) else \"\"\n",
    "    wording = str(row['Control Wording']) if not pd.isna(row['Control Wording']) else \"\"\n",
    "    anchor_text = f\"{title}: {wording}\".strip()\n",
    "    anchor_text = re.sub(r'\\s+', ' ', anchor_text)\n",
    "    \n",
    "    # 2. Raccolta Target IDs da tutte e 3 le colonne\n",
    "    target_ids_list = []\n",
    "    for col in target_columns:\n",
    "        if col in row and not pd.isna(row[col]):\n",
    "            # Split per virgola o newline\n",
    "            raw_val = str(row[col]).replace('\\n', ',')\n",
    "            codes = [c.strip().lower() for c in raw_val.split(',')]\n",
    "            target_ids_list.extend(codes)\n",
    "            \n",
    "    # 3. Matching\n",
    "    for t_id in set(target_ids_list): # set() rimuove duplicati tra le colonne\n",
    "        if not t_id: continue\n",
    "        \n",
    "        # Correzione typo comune (visto nel file Cisco: OSI -> OIS)\n",
    "        if t_id.startswith('osi-'):\n",
    "            t_id = t_id.replace('osi-', 'ois-')\n",
    "\n",
    "        if t_id in eucs_lookup:\n",
    "            # MATCH TROVATO\n",
    "            positive_text = eucs_lookup[t_id]\n",
    "            \n",
    "            training_data.append({\n",
    "                'anchor_id': anchor_id,\n",
    "                'anchor_text': anchor_text,\n",
    "                'target_id': t_id,\n",
    "                'target_text': positive_text,\n",
    "                'source': 'Cisco',\n",
    "                'target': 'EUCS'\n",
    "            })\n",
    "            stats[\"matches\"] += 1\n",
    "        else:\n",
    "            stats[\"missing\"].add(t_id)\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. SALVATAGGIO FILE STEP 4\n",
    "# ==============================================================================\n",
    "df_result = pd.DataFrame(training_data)\n",
    "df_result.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(\"RISULTATO STEP 4\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Nuove coppie generate: {len(df_result)}\")\n",
    "print(f\"Codici EUCS non trovati: {len(stats['missing'])}\")\n",
    "if stats['missing']:\n",
    "    print(f\"Esempio mancanti: {list(stats['missing'])[:5]}...\")\n",
    "print(f\"\\nFile salvato in: {OUTPUT_FILE}\")\n",
    "\n",
    "# Anteprima\n",
    "print(\"\\nAnteprima dati:\")\n",
    "print(df_result[['anchor_id', 'target_id', 'target_text']].head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9b4604",
   "metadata": {},
   "source": [
    "### NewEucs and Fabasoft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a15ffae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Caricamento EUCS e indicizzazione via BSI C5...\n",
      "   - EUCS caricato: 529 righe.\n",
      "   - Colonne trovate: ['EUCS Category', 'EUCS Control (2022)', 'Control_Code', 'EUCS Ref (Detailed)', 'EUCS Text', 'EUCS Ass. Level', 'Code', 'C5.2020 GERMANY', 'SecNumCloud FRANCE', 'ISO 27002', 'ISO 27017']\n",
      "   - Indicizzati 110 codici BSI che puntano a 576 requisiti EUCS.\n",
      "\n",
      "2. Elaborazione Fabasoft Metrics -> EUCS (via BSI)...\n",
      "   - Fabasoft caricato: 57 righe.\n",
      "   - Colonna Riferimenti Fabasoft: 'Possible Control ID\n",
      "Scheme'\n",
      "------------------------------\n",
      "RISULTATO STEP 5\n",
      "------------------------------\n",
      "Nuove coppie generate: 206\n",
      "Codici BSI in Fabasoft non mappati in EUCS: 1\n",
      "\n",
      "File salvato in: TrainAndTestData/trainingData/training_data_step5_fabasoft_eucs.csv\n",
      "                         anchor_id target_id\n",
      "0  NumberOfKnownLowVulnerabilities  OPS-17.1\n",
      "1  NumberOfKnownLowVulnerabilities  OPS-17.2\n",
      "2  NumberOfKnownLowVulnerabilities  OPS-17.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURAZIONE PERCORSI\n",
    "# ==============================================================================\n",
    "PATH_EUCS = \"Schemes/NewEucsRequirements_with_texts.csv\"\n",
    "PATH_FABASOFT = \"Schemes/fabasoftMetrics.csv\"\n",
    "OUTPUT_FILE = \"TrainAndTestData/trainingData/training_data_step5_fabasoft_eucs.csv\"\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CARICAMENTO E INDICIZZAZIONE EUCS (VIA BSI C5)\n",
    "# ==============================================================================\n",
    "print(\"1. Caricamento EUCS e indicizzazione via BSI C5...\")\n",
    "\n",
    "df_eucs = pd.DataFrame()\n",
    "\n",
    "# Lettura file con pulizia colonne\n",
    "try:\n",
    "    # Usiamo engine python per gestire CSV complessi\n",
    "    df_eucs = pd.read_csv(PATH_EUCS, sep=',', encoding='utf-8', engine='python', on_bad_lines='skip')\n",
    "    \n",
    "    # --- FIX CRUCIALE: RIMUOVIAMO SPAZI DAI NOMI DELLE COLONNE ---\n",
    "    df_eucs.columns = df_eucs.columns.str.strip()\n",
    "    \n",
    "    print(f\"   - EUCS caricato: {len(df_eucs)} righe.\")\n",
    "    print(f\"   - Colonne trovate: {df_eucs.columns.tolist()}\") # Debug\n",
    "except Exception as e:\n",
    "    print(f\"   - Errore caricamento EUCS (utf-8): {e}\")\n",
    "    # Fallback latin-1 e separatore ;\n",
    "    try:\n",
    "        df_eucs = pd.read_csv(PATH_EUCS, sep=';', encoding='latin-1', engine='python', on_bad_lines='skip')\n",
    "        df_eucs.columns = df_eucs.columns.str.strip()\n",
    "        print(f\"   - EUCS caricato (latin-1, ;): {len(df_eucs)} righe.\")\n",
    "    except Exception as e2:\n",
    "        print(f\"   - ERRORE CRITICO FILE EUCS: {e2}\")\n",
    "\n",
    "# Individuazione colonne target dopo lo strip\n",
    "col_bsi = 'C5.2020 GERMANY'\n",
    "col_eucs_id = 'EUCS Ref (Detailed)'\n",
    "col_eucs_text = 'EUCS Text'\n",
    "\n",
    "# Verifica esistenza colonna BSI\n",
    "if col_bsi not in df_eucs.columns:\n",
    "    print(f\"   - ATTENZIONE: Colonna '{col_bsi}' non trovata anche dopo la pulizia!\")\n",
    "    # Tentativo di trovarla parzialmente\n",
    "    candidates = [c for c in df_eucs.columns if \"C5\" in c and \"GERMANY\" in c]\n",
    "    if candidates:\n",
    "        col_bsi = candidates[0]\n",
    "        print(f\"   - Usando colonna alternativa: '{col_bsi}'\")\n",
    "\n",
    "bsi_to_eucs = {}\n",
    "count_mapped = 0\n",
    "\n",
    "if not df_eucs.empty and col_bsi in df_eucs.columns:\n",
    "    for _, row in df_eucs.iterrows():\n",
    "        # Estrazione dati EUCS\n",
    "        e_id = str(row[col_eucs_id]).strip() if col_eucs_id in row and not pd.isna(row[col_eucs_id]) else \"\"\n",
    "        e_text = str(row[col_eucs_text]).strip() if col_eucs_text in row and not pd.isna(row[col_eucs_text]) else \"\"\n",
    "        \n",
    "        if not e_id or not e_text: \n",
    "            continue\n",
    "            \n",
    "        # Estrazione dati BSI (Link)\n",
    "        bsi_refs = str(row[col_bsi]) if not pd.isna(row[col_bsi]) else \"\"\n",
    "        if not bsi_refs: \n",
    "            continue\n",
    "            \n",
    "        # Split per newline o virgola (es. \"OIS-01\\nOIS-03\")\n",
    "        refs = re.split(r'[\\n,]', bsi_refs)\n",
    "        \n",
    "        for r in refs:\n",
    "            clean_bsi = r.strip().lower()\n",
    "            if clean_bsi:\n",
    "                if clean_bsi not in bsi_to_eucs:\n",
    "                    bsi_to_eucs[clean_bsi] = []\n",
    "                bsi_to_eucs[clean_bsi].append((e_id, e_text))\n",
    "                count_mapped += 1\n",
    "\n",
    "print(f\"   - Indicizzati {len(bsi_to_eucs)} codici BSI che puntano a {count_mapped} requisiti EUCS.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. ELABORAZIONE FABASOFT METRICS\n",
    "# ==============================================================================\n",
    "print(\"\\n2. Elaborazione Fabasoft Metrics -> EUCS (via BSI)...\")\n",
    "\n",
    "try:\n",
    "    df_fab = pd.read_csv(PATH_FABASOFT, engine='python', on_bad_lines='skip')\n",
    "    print(f\"   - Fabasoft caricato: {len(df_fab)} righe.\")\n",
    "except Exception as e:\n",
    "    print(f\"   - ERRORE Fabasoft: {e}\")\n",
    "    df_fab = pd.DataFrame()\n",
    "\n",
    "# Trova colonna riferimenti (contiene \"Possible Control ID\")\n",
    "ref_col = None\n",
    "for col in df_fab.columns:\n",
    "    if \"Possible Control ID\" in col:\n",
    "        ref_col = col\n",
    "        break\n",
    "if not ref_col and not df_fab.empty:\n",
    "    ref_col = df_fab.columns[-1] # Fallback\n",
    "\n",
    "print(f\"   - Colonna Riferimenti Fabasoft: '{ref_col}'\")\n",
    "\n",
    "training_data = []\n",
    "stats = {\"matches\": 0, \"missing_bsi_link\": set()}\n",
    "\n",
    "if not df_fab.empty and ref_col and bsi_to_eucs:\n",
    "    for idx, row in df_fab.iterrows():\n",
    "        # Anchor (Metric)\n",
    "        m_id = str(row.get('ID', ''))\n",
    "        m_name = str(row.get('Name', ''))\n",
    "        m_desc = str(row.get('Description', ''))\n",
    "        \n",
    "        # Pulizia nan\n",
    "        if m_id.lower() == 'nan': m_id = \"\"\n",
    "        if m_name.lower() == 'nan': m_name = \"\"\n",
    "        if m_desc.lower() == 'nan': m_desc = \"\"\n",
    "        \n",
    "        anchor_parts = [p for p in [m_id, m_name, m_desc] if p]\n",
    "        anchor_text = \": \".join(anchor_parts)\n",
    "        anchor_text = re.sub(r'\\s+', ' ', anchor_text).strip()\n",
    "        \n",
    "        if not anchor_text: continue\n",
    "        \n",
    "        # Estrazione Referenze BSI da Fabasoft\n",
    "        raw_refs = str(row[ref_col]) if not pd.isna(row[ref_col]) else \"\"\n",
    "        lines = raw_refs.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            target_bsi = None\n",
    "            \n",
    "            # Cerca \"BSI C5 OPS-22\"\n",
    "            if \"BSI C5\" in line:\n",
    "                target_bsi = re.sub(r'BSI\\s+C5\\s+', '', line, flags=re.IGNORECASE).strip()\n",
    "            \n",
    "            if target_bsi:\n",
    "                clean_bsi = target_bsi.lower()\n",
    "                \n",
    "                # BRIDGE: Fabasoft(BSI) -> EUCS(BSI)\n",
    "                if clean_bsi in bsi_to_eucs:\n",
    "                    eucs_targets = bsi_to_eucs[clean_bsi]\n",
    "                    for (e_id, e_text) in eucs_targets:\n",
    "                        training_data.append({\n",
    "                            'anchor_id': m_id,\n",
    "                            'anchor_text': anchor_text,\n",
    "                            'target_id': e_id,\n",
    "                            'target_text': e_text,\n",
    "                            'source': 'Fabasoft Metrics',\n",
    "                            'target': 'EUCS (via BSI C5)'\n",
    "                        })\n",
    "                        stats[\"matches\"] += 1\n",
    "                else:\n",
    "                    stats[\"missing_bsi_link\"].add(clean_bsi)\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. SALVATAGGIO\n",
    "# ==============================================================================\n",
    "df_result = pd.DataFrame(training_data, columns=['anchor_id', 'anchor_text', 'target_id', 'target_text', 'source', 'target'])\n",
    "df_result.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(\"RISULTATO STEP 5\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Nuove coppie generate: {len(df_result)}\")\n",
    "if stats['missing_bsi_link']:\n",
    "    print(f\"Codici BSI in Fabasoft non mappati in EUCS: {len(stats['missing_bsi_link'])}\")\n",
    "print(f\"\\nFile salvato in: {OUTPUT_FILE}\")\n",
    "\n",
    "if not df_result.empty:\n",
    "    print(df_result[['anchor_id', 'target_id']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ffead3",
   "metadata": {},
   "source": [
    "### BSIC5 and Fabasoft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e09564b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Caricamento e indicizzazione BSI C5...\n",
      "   - BSI C5 JSON caricato. Totale elementi: 294\n",
      "   - Indicizzati 223 controlli BSI con testo.\n",
      "\n",
      "2. Elaborazione Fabasoft Metrics -> BSI C5 (Direct)...\n",
      "   - Fabasoft caricato: 57 righe.\n",
      "   - Colonna Riferimenti Fabasoft: 'Possible Control ID\n",
      "Scheme'\n",
      "------------------------------\n",
      "RISULTATO STEP 6\n",
      "------------------------------\n",
      "Nuove coppie generate: 27\n",
      "Codici BSI mancanti: 1 (es: ['idm-11'])\n",
      "\n",
      "File salvato in: TrainAndTestData/trainingData/training_data_step6_fabasoft_bsi.csv\n",
      "                            anchor_id target_id\n",
      "0     NumberOfKnownLowVulnerabilities    ops-22\n",
      "1     NumberOfKnownLowVulnerabilities    pss-02\n",
      "2  NumberOfKnownMediumVulnerabilities    ops-22\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURAZIONE PERCORSI\n",
    "# ==============================================================================\n",
    "PATH_FABASOFT = \"Schemes/fabasoftMetrics.csv\"\n",
    "PATH_BSI = \"Schemes/BSI-C5.json\"\n",
    "OUTPUT_FILE = \"TrainAndTestData/trainingData/training_data_step6_fabasoft_bsi.csv\"\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CARICAMENTO E INDICIZZAZIONE BSI C5 (JSON)\n",
    "# ==============================================================================\n",
    "print(\"1. Caricamento e indicizzazione BSI C5...\")\n",
    "\n",
    "try:\n",
    "    with open(PATH_BSI, 'r', encoding='utf-8') as f:\n",
    "        bsi_data = json.load(f)\n",
    "    print(f\"   - BSI C5 JSON caricato. Totale elementi: {len(bsi_data)}\")\n",
    "except Exception as e:\n",
    "    print(f\"   - ERRORE CRITICO BSI: {e}\")\n",
    "    exit()\n",
    "\n",
    "bsi_lookup = {}\n",
    "\n",
    "for item in bsi_data:\n",
    "    if 'code' not in item or not item['code']:\n",
    "        continue\n",
    "        \n",
    "    # Normalizzazione ID (es. \"OPS-22\" -> \"ops-22\")\n",
    "    code = str(item['code']).strip().lower()\n",
    "    \n",
    "    # Costruzione Testo Ricco\n",
    "    name = item.get('name', '') or \"\"\n",
    "    desc_title = item.get('descriptionTitle', '') or \"\"\n",
    "    desc_ext = item.get('descriptionExtended', '') or \"\"\n",
    "    \n",
    "    full_text = f\"{name}: {desc_title} {desc_ext}\".strip()\n",
    "    full_text = re.sub(r'\\s+', ' ', full_text).strip()\n",
    "    \n",
    "    if full_text:\n",
    "        bsi_lookup[code] = full_text\n",
    "\n",
    "print(f\"   - Indicizzati {len(bsi_lookup)} controlli BSI con testo.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. ELABORAZIONE FABASOFT METRICS -> BSI DIRECT\n",
    "# ==============================================================================\n",
    "print(\"\\n2. Elaborazione Fabasoft Metrics -> BSI C5 (Direct)...\")\n",
    "\n",
    "try:\n",
    "    # Engine python per gestire header multi-riga e quote\n",
    "    df_fab = pd.read_csv(PATH_FABASOFT, engine='python', on_bad_lines='skip')\n",
    "    print(f\"   - Fabasoft caricato: {len(df_fab)} righe.\")\n",
    "except Exception as e:\n",
    "    print(f\"   - ERRORE Fabasoft: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Trova la colonna giusta (quella che contiene \"Possible Control ID\")\n",
    "ref_col = None\n",
    "for col in df_fab.columns:\n",
    "    if \"Possible Control ID\" in col:\n",
    "        ref_col = col\n",
    "        break\n",
    "# Fallback\n",
    "if not ref_col and not df_fab.empty:\n",
    "    ref_col = df_fab.columns[-1]\n",
    "\n",
    "print(f\"   - Colonna Riferimenti Fabasoft: '{ref_col}'\")\n",
    "\n",
    "training_data = []\n",
    "stats = {\"matches\": 0, \"missing\": set()}\n",
    "\n",
    "if not df_fab.empty and ref_col:\n",
    "    for idx, row in df_fab.iterrows():\n",
    "        # 1. Anchor (Metrica)\n",
    "        m_id = str(row.get('ID', ''))\n",
    "        m_name = str(row.get('Name', ''))\n",
    "        m_desc = str(row.get('Description', ''))\n",
    "        \n",
    "        # Pulizia stringhe \"nan\"\n",
    "        if m_id.lower() == 'nan': m_id = \"\"\n",
    "        if m_name.lower() == 'nan': m_name = \"\"\n",
    "        if m_desc.lower() == 'nan': m_desc = \"\"\n",
    "        \n",
    "        anchor_parts = [p for p in [m_id, m_name, m_desc] if p]\n",
    "        anchor_text = \": \".join(anchor_parts)\n",
    "        anchor_text = re.sub(r'\\s+', ' ', anchor_text).strip()\n",
    "        \n",
    "        if not anchor_text: continue\n",
    "        \n",
    "        # 2. Estrazione Riferimenti BSI\n",
    "        raw_refs = str(row[ref_col]) if not pd.isna(row[ref_col]) else \"\"\n",
    "        \n",
    "        # Il contenuto è tipo \"BSI C5 OPS-22\\nBSI C5 PSS-02\"\n",
    "        lines = raw_refs.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            \n",
    "            # Regex per estrarre il codice dopo \"BSI C5\"\n",
    "            # Cerca \"BSI C5\" (case insensitive) seguito da spazi opzionali e poi il codice\n",
    "            match = re.search(r'BSI\\s*C5\\s*([\\w-]+)', line, re.IGNORECASE)\n",
    "            \n",
    "            if match:\n",
    "                target_code = match.group(1).lower() # es. \"ops-22\"\n",
    "                \n",
    "                # 3. Matching\n",
    "                if target_code in bsi_lookup:\n",
    "                    positive_text = bsi_lookup[target_code]\n",
    "                    \n",
    "                    training_data.append({\n",
    "                        'anchor_id': m_id,\n",
    "                        'anchor_text': anchor_text,\n",
    "                        'target_id': target_code,\n",
    "                        'target_text': positive_text,\n",
    "                        'source': 'Fabasoft Metrics',\n",
    "                        'target': 'BSI C5'\n",
    "                    })\n",
    "                    stats[\"matches\"] += 1\n",
    "                else:\n",
    "                    stats[\"missing\"].add(target_code)\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. SALVATAGGIO\n",
    "# ==============================================================================\n",
    "df_result = pd.DataFrame(training_data)\n",
    "df_result.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(\"RISULTATO STEP 6\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Nuove coppie generate: {len(df_result)}\")\n",
    "if stats['missing']:\n",
    "    print(f\"Codici BSI mancanti: {len(stats['missing'])} (es: {list(stats['missing'])[:3]})\")\n",
    "print(f\"\\nFile salvato in: {OUTPUT_FILE}\")\n",
    "\n",
    "if not df_result.empty:\n",
    "    print(df_result[['anchor_id', 'target_id']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83633965",
   "metadata": {},
   "source": [
    "### newEUCS and BSIC5 no old data overlap from oldEUCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21be404f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Caricamento Old EUCS (Blacklist)...\n",
      "   - Trovati 70 codici 'Old EUCS' da escludere dal training.\n",
      "\n",
      "2. Indicizzazione BSI C5...\n",
      "   - Indicizzati 223 controlli BSI.\n",
      "\n",
      "3. Generazione coppie NewEUCS -> BSI (Filtrate)...\n",
      "------------------------------\n",
      "RISULTATO STEP 7 (FILTRATO)\n",
      "------------------------------\n",
      "Controlli Old EUCS trovati (Blacklist): 70\n",
      "Coppie potenziali scartate perché nel Test Set: 0\n",
      "Nuove coppie valide generate: 569\n",
      "File salvato in: TrainAndTestData/trainingData/training_data_step7_neweucs_bsi_filtered.csv\n",
      "\n",
      "Anteprima:\n",
      "  anchor_id target_id\n",
      "0  OIS-01.1    ois-01\n",
      "1  OIS-01.2    ois-01\n",
      "2  OIS-01.3    ois-01\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURAZIONE PERCORSI\n",
    "# ==============================================================================\n",
    "PATH_NEW_EUCS = \"Schemes/NewEucsRequirements_with_texts.csv\"\n",
    "PATH_OLD_EUCS = \"Schemes/OldEucsRequirements.csv\"\n",
    "PATH_BSI = \"Schemes/BSI-C5.json\"\n",
    "OUTPUT_FILE = \"TrainAndTestData/trainingData/training_data_step7_neweucs_bsi_filtered.csv\"\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CARICAMENTO \"OLD EUCS\" (DA ESCLUDERE)\n",
    "# ==============================================================================\n",
    "print(\"1. Caricamento Old EUCS (Blacklist)...\")\n",
    "old_eucs_codes = set()\n",
    "\n",
    "try:\n",
    "    # Provo a leggere il vecchio file\n",
    "    df_old = pd.read_csv(PATH_OLD_EUCS)\n",
    "    \n",
    "    # Identifico la colonna ID (dal tuo head sembra 'controlId')\n",
    "    col_id_old = None\n",
    "    for c in df_old.columns:\n",
    "        if 'controlId' in c or 'ID' in c:\n",
    "            col_id_old = c\n",
    "            break\n",
    "            \n",
    "    if col_id_old:\n",
    "        # Normalizzo e salvo nel set\n",
    "        old_eucs_codes = set(df_old[col_id_old].dropna().astype(str).str.strip().str.lower())\n",
    "        print(f\"   - Trovati {len(old_eucs_codes)} codici 'Old EUCS' da escludere dal training.\")\n",
    "    else:\n",
    "        print(\"   - ATTENZIONE: Colonna ID non trovata in Old EUCS. Nessun filtro applicato!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   - Errore caricamento Old EUCS: {e}\")\n",
    "    print(\"   - Procedo senza filtrare (rischio data leakage se questo file serviva per il test).\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. INDICIZZAZIONE BSI C5 (TARGET)\n",
    "# ==============================================================================\n",
    "print(\"\\n2. Indicizzazione BSI C5...\")\n",
    "bsi_lookup = {}\n",
    "\n",
    "try:\n",
    "    with open(PATH_BSI, 'r', encoding='utf-8') as f:\n",
    "        bsi_data = json.load(f)\n",
    "        \n",
    "    for item in bsi_data:\n",
    "        if 'code' not in item: continue\n",
    "        \n",
    "        # ID BSI normalizzato\n",
    "        code = str(item['code']).strip().lower()\n",
    "        \n",
    "        # Testo Ricco\n",
    "        name = item.get('name', '') or \"\"\n",
    "        desc_t = item.get('descriptionTitle', '') or \"\"\n",
    "        desc_e = item.get('descriptionExtended', '') or \"\"\n",
    "        \n",
    "        full_text = f\"{name}: {desc_t} {desc_e}\".strip()\n",
    "        full_text = re.sub(r'\\s+', ' ', full_text).strip()\n",
    "        \n",
    "        if full_text:\n",
    "            bsi_lookup[code] = full_text\n",
    "\n",
    "    print(f\"   - Indicizzati {len(bsi_lookup)} controlli BSI.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   - ERRORE CRITICO BSI: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. MAPPING NEW_EUCS -> BSI (CON FILTRO)\n",
    "# ==============================================================================\n",
    "print(\"\\n3. Generazione coppie NewEUCS -> BSI (Filtrate)...\")\n",
    "\n",
    "training_data = []\n",
    "stats = {\n",
    "    \"total_candidates\": 0,\n",
    "    \"excluded_by_filter\": 0,\n",
    "    \"matches\": 0,\n",
    "    \"missing_bsi\": 0\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Caricamento New EUCS con pulizia colonne\n",
    "    df_new = pd.read_csv(PATH_NEW_EUCS, sep=',', engine='python', on_bad_lines='skip')\n",
    "    # Se fallisce virgola, prova ;\n",
    "    if 'C5.2020 GERMANY' not in df_new.columns and len(df_new.columns) < 5:\n",
    "         df_new = pd.read_csv(PATH_NEW_EUCS, sep=';', engine='python', on_bad_lines='skip')\n",
    "         \n",
    "    df_new.columns = df_new.columns.str.strip() # Rimuove spazi dai nomi colonne\n",
    "\n",
    "    # Colonne chiave\n",
    "    col_new_id = 'EUCS Ref (Detailed)' # ID univoco del controllo EUCS\n",
    "    col_new_text = 'EUCS Text'\n",
    "    col_bsi_ref = 'C5.2020 GERMANY'\n",
    "\n",
    "    # Verifica colonne\n",
    "    if col_new_id not in df_new.columns or col_bsi_ref not in df_new.columns:\n",
    "        print(f\"   - Colonne mancanti in New EUCS. Trovate: {df_new.columns.tolist()}\")\n",
    "        exit()\n",
    "\n",
    "    for idx, row in df_new.iterrows():\n",
    "        # 1. Analisi Anchor (EUCS)\n",
    "        eucs_id_raw = str(row[col_new_id])\n",
    "        eucs_id_clean = eucs_id_raw.strip().lower()\n",
    "        \n",
    "        eucs_text = str(row[col_new_text]) if col_new_text in row else \"\"\n",
    "        eucs_text = re.sub(r'\\s+', ' ', eucs_text).strip()\n",
    "\n",
    "        if pd.isna(row[col_new_id]) or not eucs_text:\n",
    "            continue\n",
    "\n",
    "        # 2. FILTRO ANTI-LEAKAGE\n",
    "        # Se questo ID EUCS è presente nel vecchio set (OldEucs), SALTALO.\n",
    "        if eucs_id_clean in old_eucs_codes:\n",
    "            stats[\"excluded_by_filter\"] += 1\n",
    "            continue\n",
    "\n",
    "        # 3. Analisi Target (BSI)\n",
    "        bsi_refs_raw = str(row[col_bsi_ref])\n",
    "        if pd.isna(row[col_bsi_ref]) or not bsi_refs_raw.strip():\n",
    "            continue\n",
    "\n",
    "        # Split refs (newline o virgola)\n",
    "        refs = re.split(r'[\\n,]', bsi_refs_raw)\n",
    "        \n",
    "        for r in refs:\n",
    "            stats[\"total_candidates\"] += 1\n",
    "            target_bsi = r.strip().lower()\n",
    "            \n",
    "            if not target_bsi: continue\n",
    "\n",
    "            if target_bsi in bsi_lookup:\n",
    "                # MATCH!\n",
    "                target_text = bsi_lookup[target_bsi]\n",
    "                \n",
    "                training_data.append({\n",
    "                    'anchor_id': eucs_id_raw, # Mantengo ID originale per leggibilità\n",
    "                    'anchor_text': eucs_text,\n",
    "                    'target_id': target_bsi,\n",
    "                    'target_text': target_text,\n",
    "                    'source': 'NewEUCS (Filtered)',\n",
    "                    'target': 'BSI C5'\n",
    "                })\n",
    "                stats[\"matches\"] += 1\n",
    "            else:\n",
    "                stats[\"missing_bsi\"] += 1\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   - ERRORE durante il processing: {e}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. SALVATAGGIO\n",
    "# ==============================================================================\n",
    "df_result = pd.DataFrame(training_data)\n",
    "df_result.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(\"RISULTATO STEP 7 (FILTRATO)\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Controlli Old EUCS trovati (Blacklist): {len(old_eucs_codes)}\")\n",
    "print(f\"Coppie potenziali scartate perché nel Test Set: {stats['excluded_by_filter']}\")\n",
    "print(f\"Nuove coppie valide generate: {len(df_result)}\")\n",
    "print(f\"File salvato in: {OUTPUT_FILE}\")\n",
    "\n",
    "if not df_result.empty:\n",
    "    print(\"\\nAnteprima:\")\n",
    "    print(df_result[['anchor_id', 'target_id']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3e13f6",
   "metadata": {},
   "source": [
    "## Merge to creare train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3681397d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inizio creazione del MASTER DATASET COMPLETO...\n",
      "   - Lettura: training_data_step1_cisco_ens.csv ... OK (318 righe)\n",
      "   - Lettura: training_data_step2_cisco_secNumCloud.csv ... OK (567 righe)\n",
      "   - Lettura: training_data_step3_cisco_bsi.csv ... OK (555 righe)\n",
      "   - Lettura: training_data_step4_cisco_eucs.csv ... OK (1272 righe)\n",
      "   - Lettura: training_data_step5_fabasoft_eucs.csv ... OK (206 righe)\n",
      "   - Lettura: training_data_step6_fabasoft_bsi.csv ... OK (27 righe)\n",
      "   - Lettura: training_data_step7_neweucs_bsi_filtered.csv ... OK (569 righe)\n",
      "\n",
      "Mescolamento e Pulizia Finale...\n",
      "   - Rimossi 6 duplicati esatti.\n",
      "------------------------------\n",
      "RISULTATO FINALE (MASTER DATASET)\n",
      "------------------------------\n",
      "Totale righe nel dataset di training: 3508\n",
      "\n",
      "Distribuzione per Schema Target:\n",
      "target\n",
      "EUCS                 1270\n",
      "BSI C5               1151\n",
      "SecNumCloud (EN)      563\n",
      "Spanish ENS (EN)      318\n",
      "EUCS (via BSI C5)     206\n",
      "Name: count, dtype: int64\n",
      "\n",
      "File MASTER salvato in: TrainAndTestData/trainingData/MASTER_TRAINING_DATA.csv\n",
      "\n",
      "Anteprima finale:\n",
      "  source            target anchor_id target_id\n",
      "0  Cisco              EUCS   CCF 286   pm-02.1\n",
      "1  Cisco  Spanish ENS (EN)    CCF 59   op.pl.2\n",
      "2  Cisco  SecNumCloud (EN)    CCF 28     6.5.a\n",
      "3  Cisco  SecNumCloud (EN)     CCF 8    18.1.a\n",
      "4  Cisco              EUCS   CCF 175  iam-07.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURAZIONE\n",
    "# ==============================================================================\n",
    "DATA_DIR = \"TrainAndTestData/trainingData/\"\n",
    "OUTPUT_MASTER = \"TrainAndTestData/trainingData/MASTER_TRAINING_DATA.csv\"\n",
    "\n",
    "# Lista completa dei 7 file (Tutti gli step fatti finora)\n",
    "FILES_TO_MERGE = [\n",
    "    \"training_data_step1_cisco_ens.csv\",            # Cisco -> Spanish ENS\n",
    "    \"training_data_step2_cisco_secNumCloud.csv\",    # Cisco -> SecNumCloud\n",
    "    \"training_data_step3_cisco_bsi.csv\",            # Cisco -> BSI C5\n",
    "    \"training_data_step4_cisco_eucs.csv\",           # Cisco -> EUCS (Originale)\n",
    "    \"training_data_step5_fabasoft_eucs.csv\",        # Fabasoft -> EUCS\n",
    "    \"training_data_step6_fabasoft_bsi.csv\",         # Fabasoft -> BSI C5\n",
    "    \"training_data_step7_neweucs_bsi_filtered.csv\"  # NewEUCS -> BSI (Filtrato Old)\n",
    "]\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. UNIONE (MERGE)\n",
    "# ==============================================================================\n",
    "print(\"Inizio creazione del MASTER DATASET COMPLETO...\")\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for filename in FILES_TO_MERGE:\n",
    "    file_path = os.path.join(DATA_DIR, filename)\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"   - Lettura: {filename} ...\", end=\"\")\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            # Aggiungiamo colonna origine per tracciabilità\n",
    "            df['original_file'] = filename\n",
    "            df_list.append(df)\n",
    "            print(f\" OK ({len(df)} righe)\")\n",
    "        except Exception as e:\n",
    "            print(f\" ERRORE: {e}\")\n",
    "    else:\n",
    "        print(f\"   - ATTENZIONE: File mancante: {filename}\")\n",
    "\n",
    "if not df_list:\n",
    "    print(\"ERRORE: Nessun file trovato!\")\n",
    "    exit()\n",
    "\n",
    "# Concatenazione\n",
    "master_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. SHUFFLE E PULIZIA\n",
    "# ==============================================================================\n",
    "print(\"\\nMescolamento e Pulizia Finale...\")\n",
    "\n",
    "# 1. Shuffle (Mescolamento casuale)\n",
    "# shuffle al 100% per rompere l'ordine dei file\n",
    "master_df = master_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# 2. Rimozione duplicati esatti\n",
    "# Se una coppia è identica (stessa ancora, stesso target), la teniamo una volta sola\n",
    "initial_len = len(master_df)\n",
    "master_df.drop_duplicates(subset=['anchor_text', 'target_text'], inplace=True)\n",
    "dedup_len = len(master_df)\n",
    "\n",
    "if initial_len != dedup_len:\n",
    "    print(f\"   - Rimossi {initial_len - dedup_len} duplicati esatti.\")\n",
    "\n",
    "# 3. Pulizia NaN\n",
    "master_df.dropna(subset=['anchor_text', 'target_text'], inplace=True)\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. SALVATAGGIO\n",
    "# ==============================================================================\n",
    "master_df.to_csv(OUTPUT_MASTER, index=False, encoding='utf-8')\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(\"RISULTATO FINALE (MASTER DATASET)\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Totale righe nel dataset di training: {len(master_df)}\")\n",
    "print(\"\\nDistribuzione per Schema Target:\")\n",
    "# Mostra quante coppie abbiamo per ogni tipo di target\n",
    "print(master_df['target'].value_counts())\n",
    "\n",
    "print(f\"\\nFile MASTER salvato in: {OUTPUT_MASTER}\")\n",
    "print(\"\\nAnteprima finale:\")\n",
    "print(master_df[['source', 'target', 'anchor_id', 'target_id']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6619f5",
   "metadata": {},
   "source": [
    "## Generate the 2 test data\n",
    "\n",
    "one is the old test data from the old paper (oldEucs and medinaMetrics) and the other (the one that is made by old data but it was not use for testing) is oldEucs and bsic5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d51364",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
